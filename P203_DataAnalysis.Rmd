---
title: "P203 Subjective Perceptual and ERP Amplitude (N170)"
author: "[Haiyang Jin](https://haiyangjin.github.io/)"
date: "`r format(Sys.time(), '%d-%m-%Y')`"
output:
  html_document:
    df_print: paged
    number_sections: true
    toc: true
    toc_depth: 4
    toc_float: 
      collapsed: true
      smooth_scroll: false
    includes:
      after_body: Utilities/footer.html
---

# General introduction

## Introduction

**Linear Mixed Model** were used.
  
## Experiment design

* **Independent variables**:
    + Stimulus Type (`NS`: *normal* vs. *scrambled*)
    + Stimulus Category (`FH`: *face* vs. *house*)
    + Hemisphere (`Hemisphere`: *left* vs. *right*)
    + Durations (`Durations`: *17*, *50*, *100* vs. *200*)

* **Dependent variables**:
    + Gaussian -- *mean* (`MeanAmp`)
    
    Hypothesis to be tested:
    Previous results showed the amplitudes of the N170 for faces with shorter duration are smaller. Two possible reasons:
    1. the amplitudes for the N170 is smaller for shorter durations. 
    2. the amplitudes are modulated by the responses (subjective perception).
    
    For 2. There are still two possibilities:
    2.1. For shorter durations, the amplitudes were (constantly) smaller for all the trials
    2.2. For shorter durations with correct judgement, the amplitudes are larger. With incorrect judgement, the amplitudes are smaller. As a result, the amplitudes of the N170 are smaller. \
    
    For 2.2, there are guessing involved. After guessing was excluded
    2.2.1, -> 2.1
      17_Key1 vs. 200_Key1
      17_Key1 vs. 17_Key2 (vs. 17_Key3???)
      
    2.2.2, -> 2.2
    
    replicate the previous results (LMM without response?)
  
    
## Preparations
```{r, echo=FALSE, include=FALSE}
# knitr::opts_chunk$set(error = TRUE)
knitr::opts_chunk$set(echo = TRUE) # show the code for this chunk
options(width = 110)  # set the maximum width of the html output
```

```{r setup and load the related libraries, message=FALSE}
## load libraries
library(tidyverse)
library(magrittr)
library(lme4)
library(lmerTest) 
library(emmeans)

# options for calculating estimated marginal means
# lmer.df = c("kenward-roger", "satterthwaite", "asymptotic")
emm_options(lmer.df = "satterthwaite", lmerTest.limit = 1e6)  # , pbkrtest.limit =, disable.pbkrtest = FALSE

```
  
```{r foldes and load R files, message=, include=FALSE}
# folders
folder.R <- "R"
folder.data <- "data"
folder.nesi <- "NeSI"
folder.output <- "output"

# load all the R files
sapply(list.files(folder.R, "*.R", full.names = TRUE, recursive = TRUE), source)

```

```{r settings}
# general setting
saveCSV <- FALSE
ratio.count <- 0.8
RT.min <- 200

# set up the theme for plot and rainclound plot
plot_theme <- {
  general_theme +
    theme(legend.position = "right")
}

erp_theme <- {
  general_theme +
    theme(legend.position = "bottom")
}

ylimit.n170 <- c(2, -4)  # y axis for plotting N170 amplitude

```


# Experiment 1 (204)
## Behavioral results
```{r load behavioral results of 204}
raw.beha.E1 <- {
  "E204_Incorrect_Behavior.csv" %>% 
    file.path(folder.data, .) %>% 
    read_csv() 
}

```


```{r remove some trials for P426 and P428}
# locate the trials for P426 and P428
raw.beha.E1 <- {
  raw.beha.E1 %>%
    mutate(isBad426428 = ((Subject == 426) & Block == 6) | (Subject == 428 & Session == 2 & scrambleVar == "S"))  # remove some trials for 426 and 428. (Something wrong happened for the two participants during recording EEG, therefore they have two parts of behavioral data)
}

head(raw.beha.E1)
```

```{r select certain rows and columns from the data, and calculate the Z value for reaction times (204)}
clean.beha.E1 <- {
  raw.beha.E1 %>%
    filter(`Procedure[Trial]` %in% c("faceProc", "houseProc") & !isBad426428) %>% # select the rows (remove the practice trials and bad trials for 426 and 428)
    select(ExperimentName, SubjCode = Subject, Block, Trial, Age, Sex, Handedness, Type = scrambleVar, Category = FH, Duration = stimDuration, ACC = Resp.ACC, Resp.RT, Resp = Resp.RESP, stimName) %>% # select and rename the columns (remove unuseful columns)
    mutate(
      SubjCode = factor(paste("P", SubjCode, sep = "")), # save SubjCode as factor
      Type = recode_factor(Type, "N" = "intact", "S" = "scrambled"), # rename the levels of Type
      Category = recode(Category, "hosue" = "house", "house" = "house", "face" = "face", .default = "face"), # rename "hosue" as "house"
      Stimuli = substr(stimName, 2, 6),
      Duration = factor(Duration) # save Duration as factor
      ) %>% 
    group_by(SubjCode, Type, Category, Duration, ACC) %>% # divide the data based on these conditions
    mutate(
      RT = Resp.RT + as.numeric(levels(Duration))[Duration],
      RT.Z = scale(RT), # calculate the Z value for reaction times within each group
      RT.Within3Z = ifelse(RT.Z <= 3 & RT.Z >= -3, 1, ifelse(RT.Z < -3 | RT.Z > 3, 0, NaN)) # if the Z values are between -3 and 3, will be marked as 1
      ) %>% 
    ungroup() # ungroup the data 
     
}

# behavior.tidyup
head(clean.beha.E1, 10)

```

```{r check the number of trials for 204}
# check the number of trials for 204
sum.count.E1 <- {
  clean.beha.E1 %>% 
    group_by(SubjCode, Type, Category, Duration) %>% 
    summarize(Count_sum = n())
}

valid.count.E1 <- {
  clean.beha.E1 %>% 
    filter(RT > RT.min) %>% 
    group_by(SubjCode, Type, Category, Duration) %>% 
    summarize(Count = n()) %>% 
    right_join(sum.count.E1, by = c("SubjCode", "Type", "Category", "Duration")) %>% 
    mutate(ratio = Count / Count_sum) %>% 
    filter(ratio > ratio.count) %$%
    SubjCode %>% 
    unique()
}


# remove the participants
clean.beha.E1 %<>% filter(SubjCode %in% valid.count.E1)

```


### Participant demographic information
```{r participant information}
subj.info.E1 <- {
  clean.beha.E1 %>% 
    select(SubjCode, Age, Sex, Handedness) %>% # select the columns of participant inforamtion
    distinct() # only keep the unique rows
}

subj.info.E1
```
There are `r nrow(subj.info.E1)` participants in total in this experiment.  \
The averaged age of this group is `r round(mean(subj.info.E1$Age), 3)` and the standard deviation of age is `r round(sd(subj.info.E1$Age))`. \
The number of female and male participants are `r sum(subj.info.E1$Sex == "female")` and `r sum(subj.info.E1$Sex == "male")` respectively. \
`r sum(subj.info.E1$Handedness == "right")` of them are right handed.

### Set the back difference coding
```{r dummy coding for acc E1}
# set the dummy coding manually 
clean.beha.E1 <- dummy_coding_P203(clean.beha.E1)

if (saveCSV) {
  output.beha.E1 <- file.path(folder.nesi, "E204_beha.RData")
  save(clean.beha.E1, file = output.beha.E1)
}
```

### Behavioral responses
#### Accuracy
```{r calculate the accuracy (E1)}
# accuracy data
avg.acc.R.E1 <- {
  clean.beha.E1 %>%
    group_by(SubjCode, Type, Duration) %>%
    summarize(Accuracy = mean(ACC)) %>%
    ungroup() 
}

# descriptive statistics of accuracy for plotting
desc.acc.R.E1 <- {
  avg.acc.R.E1 %>%
    group_by(Type, Duration) %>%
    summarize(Mean = mean(Accuracy)
              , N = n()
              , SD = sd(Accuracy)
              , SE = SD/sqrt(N)
              , SE.lo = Mean - SE, SE.hi = Mean + SE
              , CI = SE * qt(0.975,N)
              , Median = median(Accuracy)
              , Lower = Mean - SD, Upper = Mean + SD # for rainclound plot
              )  
}

# accuracy data for SPSS analysis
avg.acc.SPSS.E1 <- {
  avg.acc.R.E1 %>%
    mutate(Conditions = paste(Type, Duration, sep = ".")) %>%
    select(SubjCode, Conditions, Accuracy) %>%
    spread(Conditions, Accuracy)
}

# save the data as *.csv
if (saveCSV) {
  output.acc.SPSS.E1 <- file.path(folder.output, "E204_acc.csv")
  write.csv(avg.acc.SPSS.E1, output.acc.SPSS.E1, row.names = FALSE)
} 

```


##### RainClound plot of accuracy
```{r rainclound plot of accuracy E1, warning=FALSE}
# print(sum.acc.R, width = Inf)
knitr::kable(desc.acc.R.E1, digits = 4)

acc.RainPlot.E1 <- {
  ggplot(data = avg.acc.R.E1, aes(y = Accuracy, x = Type, fill = Duration)) + 
    geom_flat_violin(position = position_nudge(x = .2, y = 0), alpha = .7) +
    geom_point(aes(color = Duration), position = position_jitter(width = .15), size = .5, alpha = .8) +
    geom_boxplot(width = .2, outlier.shape = NA, alpha = .7) +
    geom_point(data = desc.acc.R.E1, aes(y = Mean, x = Type, color = Duration), position = position_nudge(x = 0.3), size = 2.5) +
    geom_errorbar(data = desc.acc.R.E1, aes(y = Mean, ymin = Lower, ymax = Upper), position = position_nudge(x = 0.3), width = 0) +
    # scale_colour_grey() + # start = .1, end = .6, color for the contour
    # scale_fill_grey() + # start = .3, end = .6, color for the fill
    # scale_color_brewer(palette = "Set1") +
    # scale_fill_brewer(palette = "Set1") +
    labs(title = "Accuracy for E204", x = "Stimulus Type", y = "Accuracy", fill = "Duration(ms)", color = "Duration(ms)") +  # set the names for main, x and y axises and the legend
    coord_cartesian(ylim = c(0, 1.05)) +  # set the limit for y axis
    scale_y_continuous(expand= c(0, 0)) +  # remove the space between columns and x axis
    geom_hline(yintercept = c(0.5, 1), linetype = 5, alpha = 0.5) + # add the line for 0.5 and 1 (y)
    theme_bw() + # the backgroud color
    plot_theme
}

```

&nbsp;

```{r display the RainCloud plot of accuracy}
acc.RainPlot.E1
```


##### Column plot of accuracy
```{r Plot of accuracy of behavioral data E1, warning = FALSE}

acc.ColuPlot.E1 = {
  ggplot(data = desc.acc.R.E1, aes(y = Mean, x = Type, fill = Duration )) +  # set the data, varialbes for x and y axises, and the variable for dividing data
    geom_col(position = "dodge", color = "black", alpha = .7) +  # position of columns and countour of columns
    geom_errorbar(mapping = aes(ymin = SE.lo, ymax = SE.hi), linetype = 1,  # set the error bar
                  show.legend = FALSE, width = 0.25, alpha = .5, 
                  position = position_dodge(width=0.9)) +
    geom_hline(yintercept = c(0.5, 1), linetype = 5, alpha = 0.5) +  # add the line for 0.5 and 1 (y)
    coord_cartesian(ylim = c(0,1.1)) +  # set the limit for y axis
    scale_y_continuous(expand= c(0, 0)) +  # remove the space between columns and x axis
    labs(title = "Accuracy for E204", x = "Stimulus Type", y = "Accuracy", fill = "Durations(ms)") +  # set the names for main, x and y axises
    scale_fill_grey() +  # set the color for columns
    geom_text(label = c("***", "***", "", ""), color = c("red", "red", "red", "red"), size = 6, nudge_y = 0.05, nudge_x = rep(c(-0.225, 0.225), 2)) + # add starts to the significant columns
    theme_bw() +
    plot_theme
}

acc.ColuPlot.E1
```

##### Linear mixed model for accuracy
```{r manually add dummy coding for accuracy (E1)}
avg.acc.R.E1 <- {
  avg.acc.R.E1 %>% 
    mutate(Type_D = ifelse(Type == "normal", 0, 1),
           Dura_D = ifelse(Duration == "17", 0, 1),
           
           Type_Dura = Type_D * Dura_D)
}
```



```{r}
lmm.max.acc.E1 <- lmer(Accuracy ~ Type * Duration + (1 + Type_D + Dura_D | SubjCode),
                       data = sum.acc.R.E1,
                       REML = FALSE,
                       control = lmerControl(optimizer = "bobyqa"))

summary(lmm.max.acc.E1)
```

```{r}
lmm.zcp.acc.E1 <- update(lmm.max.acc.E1,
                         formula =  Accuracy ~ Type * Duration + (1 + Type_D + Dura_D || SubjCode))

summary(lmm.zcp.acc.E1)
```

```{r}
lmm.rdc.acc.E1 <- update(lmm.zcp.acc.E1, 
                         formula = Accuracy ~ Type * Duration + (1 | SubjCode))

summary(lmm.rdc.acc.E1)
```

```{r}
anova(lmm.zcp.acc.E1, lmm.rdc.acc.E1)
```

```{r}
lmm.rdc1.acc.E1 <- update(lmm.zcp.acc.E1,
                          formula = Accuracy ~ Type * Duration + (1 + Type_Dura || SubjCode))

summary(lmm.rdc1.acc.E1)
```

```{r}
anova(lmm.rdc1.acc.E1, lmm.rdc.acc.E1)
```

```{r}
lmm.opt.acc.E1 <- update(lmm.rdc.acc.E1,
                         REML = TRUE)

summary(lmm.opt.acc.E1)
```

```{r}
# qqplot_lmer(lmm.opt.acc.E1)

```




```{r}
emm.acc.E1 <- emmeans(lmm.opt.acc.E1, ~ Duration | Type)

data.frame(emm.acc.E1)
```

```{r}
emmip(lmm.opt.acc.E1, ~ Duration | Type, CIs = TRUE)
```

```{r}
contrast(emm.acc.E1, "pairwise", simple = "each", combine = TRUE)
```

```{r}
test(emm.acc.E1, null = 0.5, side = ">")

```


#### Hits
```{r calculate the hits (and save the data into csv) E1}

# hit data for R analysis
sum.hit.R.E1 <- {
  clean.beha.E1 %>%
    group_by(SubjCode, Type, Category, Duration) %>%
    summarize(Hit = mean(ACC), count = n()) %>%
    ungroup()
}

# descriptive statistics of hits for plotting
desc.hit.R.E1 <- {
  sum.hit.R.E1 %>% 
    group_by(Type, Category, Duration) %>% 
    summarize(Mean = mean(Hit), N = n(), SD = sd(Hit), SE = SD/sqrt(N), SE.hi = Mean + SE, SE.lo = Mean - SE, CI = SE * qt(0.975,N), Median = median(Hit), Lower = Mean - SD, Upper = Mean + SD)
}

# hit data for SPSS analysis
sum.hit.SPSS.E1 <- {
  sum.hit.R.E1 %>%
    mutate(Conditions = paste(Type, Category, Duration, sep = ".")) %>%
    select(SubjCode, Conditions, Hit) %>%
    spread(Conditions, Hit)
}

# save the data as *.csv
if (saveCSV) {
  output.hit.SPSS.E1 <- file.path(folder.output, "E204_hit.csv")
  write.csv(sum.hit.SPSS.E1, output.hit.SPSS.E1, row.names = FALSE)
} 

```


##### RainClound plot of hits
```{r rainclound plot of hits E1}
knitr::kable(desc.hit.R.E1, digits = 4)

hit.RainPlot.E1 <- {
  ggplot(data = sum.hit.R.E1, aes(y = Hit, x = Category, fill = Duration)) + 
    geom_flat_violin(position = position_nudge(x = .2, y = 0), alpha = .7) +
    geom_point(aes(y = Hit, color = Duration), position = position_jitter(width = .15), size = .5, alpha = .8) +
    geom_boxplot(aes(y = Hit), width = .2, outlier.shape = NA, alpha = .7) +
    geom_point(data = desc.hit.R.E1, aes(y = Mean, x = Category, color = Duration), position = position_nudge(x = 0.3), size = 2.5) +
    geom_errorbar(data = desc.hit.R.E1, aes(y = Mean, ymin = Lower, ymax = Upper), position = position_nudge(x = 0.3), width = 0) +
    facet_grid(. ~ Type, switch = "x") +  # create two panels to show data
    # scale_colour_grey() + # start = .1, end = .6, color for the contour
    # scale_fill_grey() + # start = .3, end = .6, color for the fill
    # scale_color_brewer(palette = "Set1") + # palette = "RdBu"
    # scale_fill_brewer(palette = "Set1") + # palette = "RdBu"
    labs(title = "Hits for E204", x = "Stimulus Type", y = "Hit Rate", fill = "Duration(ms)", color = "Duration(ms)") +  # set the names for main, x and y axises and the legend
    coord_cartesian(ylim = c(0, 1.05)) +  # set the limit for y axis
    scale_y_continuous(expand= c(0, 0)) +  # remove the space between columns and x axis
    geom_hline(yintercept = c(0.5, 1), linetype = 5, alpha = 0.5) + # add the line for 0.5 and 1 (y)
    theme_bw() + # the backgroud color
    plot_theme
}

```

&nbsp;

```{r display the RainCloud plot of hits E1}
hit.RainPlot.E1
```

##### Column plot of hits
```{r Plot of hits for behavioral data E1, warning=FALSE}

hit.ColuPlot.E1 = {
  ggplot(data = desc.hit.R.E1, aes(y = Mean, x = Category, fill = Duration )) +  # set the data, varialbes for x and y axises, and the variable for dividing data
    geom_col(position = "dodge", color = "black", alpha = .7) +  # position of columns and countour of columns
    geom_errorbar(mapping = aes(ymin = SE.lo, ymax = SE.hi), linetype = 1,  # set the error bar
                  show.legend = FALSE, width = 0.25, alpha = .5, 
                  position = position_dodge(width=0.9)) +
    facet_grid(. ~ Type, switch = "x") +  # create two panels to show data
    geom_hline(yintercept = c(0.5, 1), linetype = 5, alpha = 0.5) +  # add the line for 0.5 and 1 (y)
    coord_cartesian(ylim = c(0,1.1)) +  # set the limit for y axis
    scale_y_continuous(expand= c(0, 0)) +  # remove the space between columns and x axis
    labs(title = "Hits for E204", x = "Stimulus Type", y = "Hit Rate", fill = "Durations(ms)") +  # set the names for main, x and y axises
    scale_fill_grey() +  # set the color for columns
    # scale_fill_manual(values=c(faceColors, houseColors)) +  # set the color for columns and the label names for legend  , name = "xxx"
    geom_text(label = c("***", "***", "***", "***", "*", "", "", ""), color = rep("red", 8), size = 6, nudge_y = 0.05, nudge_x = rep(c(-0.225, 0.225), 4)) + # add starts to the significant columns
    theme_bw() +
    plot_theme
}

hit.ColuPlot.E1
```


##### Linear mixed model for hits
```{r manually add dummy coding for hit (E1)}
sum.hit.R.E1 %<>% dummy_coding_P203() 

```



```{r}
lmm.max.hit.E1 <- lmer(Hit ~ Type * Category * Duration + 
                         (1 + Type_D + Cate_D + Dura_D + Type_Cate + Type_Dura + Cate_Dura | SubjCode),
                       data = sum.hit.R.E1,
                       REML = FALSE,
                       control = lmerControl(optimizer = "bobyqa"))

summary(lmm.max.hit.E1)
```

```{r}
lmm.zcp.hit.E1 <- update(lmm.max.hit.E1,
                         formula = Hit ~ Type * Category * Duration + 
                         (1 + Type_D + Cate_D + Dura_D + Type_Cate + Type_Dura + Cate_Dura || SubjCode))

summary(lmm.zcp.hit.E1)
```

```{r}
summary(rePCA(lmm.zcp.hit.E1))

lmm.rdc.hit.E1 <- update(lmm.zcp.hit.E1,
                         formula = Hit ~ Type * Category * Duration + 
                         (0 + Cate_D + Type_Cate || SubjCode))

anova(lmm.rdc.hit.E1, lmm.zcp.hit.E1)

```

```{r}
lmm.rdc1.hit.E1 <- update(lmm.rdc.hit.E1, 
                          formula = Hit ~ Type * Category * Duration + 
                         (0 + Cate_D + Type_Cate + Type_Cate_Dura || SubjCode))

anova(lmm.rdc1.hit.E1, lmm.rdc.hit.E1)
```

```{r}
lmm.etd.hit.E1 <- update(lmm.rdc.hit.E1,
                         formula = Hit ~ Type * Category * Duration + 
                         (0 + Cate_D + Type_Cate | SubjCode))

summary(lmm.etd.hit.E1)
```

```{r}
lmm.etd1.hit.E1 <- update(lmm.rdc.hit.E1,
                         formula = Hit ~ Type * Category * Duration + 
                         (0 + Cate_D | SubjCode))

summary(lmm.etd1.hit.E1)
```

```{r}
lmm.etd2.hit.E1 <- update(lmm.rdc.hit.E1,
                         formula = Hit ~ Type * Category * Duration + 
                         (0 + Type_Cate | SubjCode))

summary(lmm.etd2.hit.E1)
```

```{r}
anova(lmm.etd1.hit.E1, lmm.etd2.hit.E1)
```

```{r}
lmm.opt.hit.E1 <- update(lmm.etd2.hit.E1,
                         REML = TRUE)

summary(lmm.opt.hit.E1)
```

```{r}
emm.hit.E1 <- emmeans(lmm.opt.hit.E1, ~ c(Category, Duration) | Type)

data.frame(emm.hit.E1)
```

```{r}
emmip(lmm.opt.hit.E1, Category ~ Duration | Type, CIs = TRUE)
```

```{r}
contrast(emm.hit.E1, "pairwise", simple = "each", combine = TRUE)
```

```{r}
test(emm.hit.E1, null = 0.5, side = "two-sided")
```


#### Logit mixed model
##### The maximal model
```{r maximal lmm for E204 responses}
file.max.resp.E1 <- file.path(folder.nesi, "E204_resp_glmm_max.RData")

if (!file.exists(file.max.resp.E1)) {
  glmm.max.resp.E1 <- glmer(ACC ~ Type * Category * Duration +
                             (1 + Type_D + Cate_D + Dura_D + Type_Cate + Type_Dura + Cate_Dura + Type_Cate_Dura | SubjCode) +
                             (1 + Type_D + Dura_D + Type_Dura | Stimuli),
                           data = clean.beha.E1,
                           family = binomial(link = "logit"),
                           control = glmerControl(optCtrl = list(maxfun = 1e6)))
} else {
  load(file.max.resp.E1)
}

print(summary(glmm.max.resp.E1), corr = FALSE)

```

```{r}
summary(rePCA(glmm.max.resp.E1))
```


##### The zero-correlation-parameter model
```{r zcp lmm for E204 Response}
file.zcp.resp.E1 <- file.path(folder.nesi, "E204_resp_glmm_zcp.RData")

if (!file.exists(file.zcp.resp.E1)) {
  glmm.zcp.resp.E1 <- update(glmm.max.resp.E1,
                           formula = ACC ~ Type * Category * Duration + 
                            (1 + Type_D + Cate_D + Dura_D + Type_Cate + Type_Dura + Cate_Dura + Type_Cate_Dura || SubjCode) +
                            (1 + Type_D + Dura_D + Type_Dura || Stimuli),
                          verbose = FALSE)
  
  glmm.zcp1.resp.E1 <- re_fit(glmm.zcp.resp.E1)
  
} else {
  load(file.zcp.resp.E1)
}

print(summary(glmm.zcp.resp.E1), corr = FALSE)

```

```{r}
print(summary(glmm.zcp1.resp.E1), corr = FALSE)

```


##### The reduced model
```{r}
summary(rePCA(glmm.zcp1.resp.E1))

```

##### Fit with all optimizers
```{r}
file.max.resp.E1.allFit <- file.path(folder.nesi, "E204_resp_glmm_max_allFit.RData")

if (!file.exists(file.max.resp.E1.allFit)) {
  glmm.max0.resp.E1 <- glmer(resp ~ Type * Category * Duration +
                             (1 + Type_D + Cate_D + Dura_D + Type_Cate + Type_Dura + Cate_Dura + Type_Cate_Dura | SubjCode) +
                             (1 + Dura_D | Stimuli),
                           data = clean.beha.E1,
                           family = "binomial")
  
  glmm.max.resp.E1.allFit = allFit(glmm.max0.resp.E1,
                                 maxfun = 1e6)
} else {
  load(file.max.resp.E1.allFit)
}

print(summary(glmm.max.resp.E1.allFit), corr = FALSE)


```


### Correct response times
```{r calculate the correct RT E1}
df.rt.E1 <- {
  clean.beha.E1 %>%
    filter((ACC == 1 | Type == "scrambled")
           & RT.Within3Z == 1 # remove the RT outsides 3Z
           )
}

sum.rt.R.E1 <- {
   df.rt.E1 %>% # only include the correct responses for normal trials but all trial for scrambled
    select(SubjCode, Type, Category, Duration, RT) %>% # select the columns
    group_by(SubjCode, Type, Category, Duration) %>% # divide data into groups to get the mean
    summarize(RT = mean(RT), count = n()) %>%  # calculate the mean of RT
    ungroup()
}

desc.rt.R.E1 <- {
  sum.rt.R.E1 %>% 
    group_by(Type, Category, Duration) %>% 
    summarize(Mean = mean(RT), N = n(), SD = sd(RT), SE = SD/sqrt(N), SE.hi = Mean + SE, SE.lo = Mean - SE, CI = SE * qt(0.975,N), Median = median(RT), Lower = Mean - SD, Upper = Mean + SD)
}

sum.rt.SPSS.E1 <- {
  sum.rt.R.E1 %>% 
    mutate(Conditions = paste(Type, Category, Duration, sep = ".")) %>%
    select(-c(Type, Category, Duration, count)) %>%
    spread(Conditions, RT)
}

 # save the data into *.csv
if (saveCSV) {
  output.rt.E1 <- file.path(folder.nesi, "E204_beha_RT.RData")
  save(df.rt.E1, file = output.rt.E1)
  output.rt.SPSS.E1 <- file.path(folder.output, "E204_RT.csv")
  write.csv(sum.rt.SPSS.E1, output.rt.SPSS.E1, row.names = FALSE)
}
```

#### RainClound plot of the response times 
```{r rainclound plot of the RT data, warning = FALSE}
knitr::kable(desc.rt.R.E1, digits = 4)

rt.RainPlot.E1 <- {
  ggplot(data = sum.rt.R.E1, aes(y = RT, x = Category, fill = Duration)) + 
    geom_flat_violin(position = position_nudge(x = .2, y = 0), alpha = .7) +
    geom_point(aes(y = RT, color = Duration), position = position_jitter(width = .15), size = .5, alpha = .8) +
    geom_boxplot(aes(y = RT), width = .2, outlier.shape = NA, alpha = .7) +
    geom_point(data = desc.rt.R.E1, aes(y = Mean, x = Category, color = Duration), position = position_nudge(x = 0.3), size = 2.5) +
    geom_errorbar(data = desc.rt.R.E1, aes(y = Mean, ymin = Lower, ymax = Upper), position = position_nudge(x = 0.3), width = 0) +
    facet_grid(. ~ Type, switch = "x") +  # create two panels to show data
    # scale_colour_grey() + # start = .1, end = .6, color for the contour
    # scale_fill_grey() + # start = .3, end = .6, color for the fill
    # scale_color_brewer(palette = "Set1") + # palette = "RdBu"
    # scale_fill_brewer(palette = "Set1") + # palette = "RdBu"
    labs(title = "Reaction times for E204", x = "Stimulus Type", y = "Reaction times (ms)", fill = "Duration(ms)", color = "Duration(ms)") +  # set the names for main, x and y axises and the legend
    # coord_cartesian(ylim = c(0, 1.05)) +  # set the limit for y axis
    # scale_y_continuous(expand= c(0, 0)) +  # remove the space between columns and x axis
    theme_bw() + # the backgroud color
    plot_theme
}

```

&nbsp;

```{r display the RainCloud plot for reaction times}
rt.RainPlot.E1
```


#### Column plot of the response times
```{r Plot of RT data}

rt.ColuPlot.E1 = {
  ggplot(data = desc.rt.R.E1, aes(y = Mean, x = Category, fill = Duration )) +  # set the data, varialbes for x and y axises, and the variable for dividing data
    geom_col(position = "dodge", color = "black", alpha = .7) +  # position of columns and countour of columns
    geom_errorbar(mapping = aes(ymin = SE.lo, ymax = SE.hi), linetype = 1,  # set the error bar
                  show.legend = FALSE, width = 0.25, alpha = .5, 
                  position = position_dodge(width=0.9)) +
    facet_grid(. ~ Type, switch = "x") +  # create two panels to show data
    coord_cartesian(ylim = c(0, 850)) +  # set the limit for y axis
    scale_y_continuous(expand= c(0, 0)) +  # remove the space between columns and x axis
    labs(title = "Reaction times for E204", x = "Stimulus Type", y = "Reaction times (ms)", fill = "Durations(ms)") +  # set the names for main, x and y axises
    scale_fill_grey() +  # set the color for columns
    # geom_text(label = c("*", "***", "", ""), color = c("red", "red", "red", "red"), size = 6, nudge_y = 0.05, nudge_x = rep(c(-0.225, 0.225), 2)) + # add starts to the significant columns
    theme_bw() +
    plot_theme
}

rt.ColuPlot.E1
```

#### Linear mixed model
##### The maximal model
```{r maximal lmm for E204 RT}
file.max.rt.E1 <- file.path(folder.nesi, "E204_rt_lmm_max.RData")

if (!file.exists(file.max.rt.E1)) {
  lmm.max.rt.E1 <- lmer(RT ~ Type * Category * Duration +
                          (1 + Type_D + Cate_D + Dura_D + Type_Cate + Type_Dura + Cate_Dura + Type_Cate_Dura | SubjCode) +
                          (1 + Type_D + Dura_D + Type_Dura | Stimuli),
                        data = df.rt.E1,
                        verbose = TRUE,
                        REML = FALSE,
                        control = lmerControl(optimizer = "bobyqa",
                                              optCtrl = list(maxfun = 1e5)))
} else {
  load(file.max.rt.E1)
}

print(summary(lmm.max.rt.E1), corr = FALSE)


```


##### The zero-correlation-parameter model
```{r zcp lmm for E204 RT}
file.zcp.rt.E1 <- file.path(folder.nesi, "E204_rt_lmm_zcp.RData")

if (!file.exists(file.zcp.rt.E1)) {
  lmm.zcp.rt.E1 <- update(lmm.max.rt.E1,
                          formula = RT ~ Type * Category * Duration +
                          (1 + Type_D + Cate_D + Dura_D + Type_Cate + Type_Dura + Cate_Dura + Type_Cate_Dura || SubjCode) +
                          (1 + Type_D + Dura_D + Type_Dura || Stimuli),
                          verbose = FALSE
                          )
} else {
  load(file.zcp.rt.E1)
}

print(summary(lmm.zcp.rt.E1), corr = FALSE)

```


##### The reduced model
```{r}
summary(rePCA(lmm.zcp.rt.E1))
```


```{r reduced lmm for E204 RT}
file.rdc.rt.E1 <- file.path(folder.nesi, "E204_rt_lmm_rdc.RData")

if (!file.exists(file.rdc.rt.E1)) {
  lmm.rdc.rt.E1 <- step(lmm.zcp.rt.E1, reduce.fixed = FALSE)
  
  lmm.rdc.rt.E1 <- update(lmm.max.rt.E1,
                          formula = RT ~ Type * Category * Duration +
                          (1 + Type_D + Cate_D + Dura_D + Type_Cate + Type_Dura + Cate_Dura + Type_Cate_Dura || SubjCode) +
                          (0 + Type_D + Type_Dura || Stimuli),
                          verbose = FALSE
                          )
} else {
  load(file.rdc.rt.E1)
}

print(summary(lmm.rdc.rt.E1), corr = FALSE)

lmm.zcp.rt.E1.step

```

```{r}
anova(lmm.zcp.rt.E1, lmm.rdc.rt.E1)
```


##### The extended model
```{r extended lmm for E204 RT}
file.etd.rt.E1 <- file.path(folder.nesi, "E204_rt_lmm_etd.RData")

if (!file.exists(file.etd.rt.E1)) {
  lmm.etd.rt.E1 <- update(lmm.rdc.rt.E1,
                          formula = RT ~ Type * Category * Duration +
                          (1 + Type_D + Cate_D + Dura_D + Type_Cate + Type_Dura + Cate_Dura + Type_Cate_Dura | SubjCode) +
                          (0 + Type_D + Type_Dura | Stimuli),
                          verbose = FALSE
                          )
} else {
  load(file.etd.rt.E1)
}

print(summary(lmm.etd.rt.E1), corr = FALSE)

```


```{r}
summary(rePCA(lmm.etd.rt.E1))
```


```{r extended1 lmm for E204 RT}
file.etd1.rt.E1 <- file.path(folder.nesi, "E204_rt_lmm_etd1.RData")

if (!file.exists(file.etd1.rt.E1)) {
  lmm.etd1.rt.E1 <- update(lmm.etd.rt.E1,
                          formula = RT ~ Type * Category * Duration +
                          (1 + Type_D + Cate_D + Dura_D + Type_Cate + Type_Dura + Type_Cate_Dura | SubjCode) +
                          (0 + Type_D + Type_Dura | Stimuli),
                          verbose = FALSE
                          )
} else {
  load(file.etd1.rt.E1)
}

print(summary(lmm.etd1.rt.E1), corr = FALSE)

```


```{r}
anova(lmm.etd1.rt.E1, lmm.rdc.rt.E1)
```

##### The optimal model
```{r}

file.opt.rt.E1 <- file.path(folder.nesi, "E204_rt_lmm_opt.RData")

if (!file.exists(file.opt.rt.E1)) {
  lmm.opt.rt.E1 <- update(lmm.etd1.rt.E1,
                          REML = TRUE,
                          verbose = FALSE)
} else {
  load(file.opt.rt.E1)
}

print(summary(lmm.opt.rt.E1), corr = FALSE)

```

##### Disgnostic plots
```{r qqplot for lmm.opt.rt.E1}
# qqplot
qqplot_lmer(lmm.opt.rt.E1)
```


#### Log mixed model
##### The maximal model
```{r maximal glmm for E204 RT}
file.max.rt.E1 <- file.path(folder.nesi, "E204_rt_glmm_max.RData")

if (!file.exists(file.max.rt.E1)) {
  glmm.max.rt.E1 <- glmer(RT ~ Type * Category * Duration +
                             (1 + Type_D + Cate_D + Dura_D + Type_Cate + Type_Dura + Cate_Dura + Type_Cate_Dura | SubjCode) +
                             (1 + Type_D + Dura_D + Type_Dura | Stimuli),
                           data = df.rt.E1,
                           family = poisson(link = "log"),
                           control = glmerControl(optimizer = "Nelder_Mead",
                                                  optCtrl = list(maxfun = 1e6)))
} else {
  load(file.max.rt.E1)
}

print(summary(glmm.max.rt.E1), corr = FALSE)
```


##### The zero-correlation-parameter model
```{r zcp glmm for E204 RT}
file.zcp.rt.E1 <- file.path(folder.nesi, "E204_rt_glmm_zcp.RData")

# fit the zcp model
if (!file.exists(file.zcp.rt.E1)) {
  glmm.zcp.rt.E1 <- update(glmm.max.rt.E1, 
                          formula = RT ~ Type * Category * Duration +
                             (1 + Type_D + Cate_D + Dura_D + Type_Cate + Type_Dura + Cate_Dura + Type_Cate_Dura || SubjCode) +
                             (1 + Dura_D || Stimuli),
                          verbose = FALSE)
} else {
  load(file.zcp.rt.E1)
}

print(summary(glmm.zcp.rt.E1), corr = FALSE)

```

```{r compare max and zcp E204 rt}
anova(glmm.max.rt.E1, glmm.zcp.rt.E1)
```

##### The reduced model
```{r PCA for zcp RT 204}
# PCA
summary(rePCA(glmm.zcp.rt.E1))
```

##### Refit the maximal model
```{r}
file.max1.rt.E1 <- file.path(folder.nesi, "E204_rt_glmm_max1.RData")

if (!file.exists(file.max1.rt.E1)) {
  glmm.max1.rt.E1 <- refit(glmm.max.rt.E1)
} else {
  load(file.max1.rt.E1)
}

print(summary(glmm.max1.rt.E1), corr = FALSE)

```



## ERP amplitude analysis
### Load mean amplitude for single trials
The raw mean amplitude of each trial for all participants were loaded.
```{r load trial mean amplitude E1, message = FALSE}
# read the trial mean amplitude data
df.erp.E1 <- {
  "E204_MeanAmp.csv" %>% 
    file.path(folder.data, .) %>% 
    read_csv() %>% 
    substr_Event() %>% 
    mutate(SubjCode = as.factor(SubjCode),
           Hemisphere = as.factor(Hemisphere),
           Response = as.factor(Response),
           Stimuli = substr(stimName, 2, 6))
}


df.erp.E1 %>% 
  group_by(SubjCode, Stimuli, Hemisphere, Type, Category, Duration) %>% 
  summarize(count = n())

```

### Set the successive difference coding
```{r set the dummy coding for ERP data E1}
# dummy coding
df.erp.E1 <- sdif_coding_E204_erp(df.erp.E1)

```

### Amplitudes of the P1
```{r erp df for the P1 E1}
# only keep the data for amplitudes of the P1 (correct and incorrect erps)
df.erp.P1.E1 <- {
  df.erp.E1 %>% 
    filter(Component == "P1") %>% 
    substr_Event() 
}

if (saveCSV) {
  output.erp.P1.E1 <- file.path(folder.nesi, "E204_erp_P1.RData")
  save(df.erp.P1.E1, file = output.erp.P1.E1)
}

# the structure of this dataset
head(df.erp.P1.E1, 10)
```


#### The maximal model
```{r maximal lmm for E204 p1}
file.max.P1.E1 <- file.path(folder.nesi, "E204_P1_lmm_max.RData")

# fit the maximal model
if (!file.exists(file.max.P1.E1)) {
  lmm.max.P1.E1 <- lmer(MeanAmp ~ Type * Category * Duration * Hemisphere * Response + 
                            (1 + Type_C + Cate_C + Dura_C + Hemi_C +  
                               Type_Cate + Type_Dura + Cate_Dura + Type_Hemi + Cate_Hemi + Dura_Hemi +
                               Type_Cate_Dura + Type_Cate_Hemi + Type_Dura_Hemi + Cate_Dura_Hemi + 
                               Type_Cate_Dura_Hemi +
                               Resp_C + 
                               Type_Resp + Cate_Resp + Dura_Resp + Hemi_Resp +
                               Type_Cate_Resp + Type_Dura_Resp + Cate_Dura_Resp + 
                               Type_Hemi_Resp + Cate_Hemi_Resp + Dura_Hemi_Resp +
                               Type_Cate_Dura_Resp + Type_Cate_Hemi_Resp + Type_Hemi_Dura_Resp + Hemi_Cate_Dura_Resp +
                               Type_Cate_Dura_Hemi_Resp
                             | SubjCode) +
                            (1 + Type_C + Dura_C + Hemi_C + 
                               Type_Dura + Type_Hemi + Dura_Hemi +
                               Type_Dura_Hemi +
                               Resp_C + 
                               Type_Resp + Dura_Resp + Hemi_Resp +
                               Type_Dura_Resp + Type_Hemi_Resp + Dura_Hemi_Resp +
                               Type_Hemi_Dura_Resp
                             | Stimuli),
                          data = df.erp.P1.E1,
                          REML = FALSE,
                          # verbose = TRUE,
                          control = lmerControl(optimizer = "bobyqa",  # nloptwrap Nelder_Mead
                                                optCtrl = list(maxfun = 1e7)))
} else {
  load(file.max.P1.E1)
}

print(summary(lmm.max.P1.E1), corr = FALSE)

```

#### The zero-correlation-parameter model
```{r zcp lmm for E1 p1}
file.zcp.P1.E1 <- file.path(folder.nesi, "E204_P1_lmm_zcp.RData")

# fit the zcp model
if (!file.exists(file.zcp.P1.E1)) {
  lmm.zcp.P1.E1 <- lmer(MeanAmp ~ Type * Category * Duration * Hemisphere * Response + 
                            (1 + Type_C + Cate_C + Dura_C + Hemi_C +  
                               Type_Cate + Type_Dura + Cate_Dura + Type_Hemi + Cate_Hemi + Dura_Hemi +
                               Type_Cate_Dura + Type_Cate_Hemi + Type_Dura_Hemi + Cate_Dura_Hemi + 
                               Type_Cate_Dura_Hemi +
                               Resp_C + 
                               Type_Resp + Cate_Resp + Dura_Resp + Hemi_Resp +
                               Type_Cate_Resp + Type_Dura_Resp + Cate_Dura_Resp + 
                               Type_Hemi_Resp + Cate_Hemi_Resp + Dura_Hemi_Resp +
                               Type_Cate_Dura_Resp + Type_Cate_Hemi_Resp + Type_Hemi_Dura_Resp + Hemi_Cate_Dura_Resp +
                               Type_Cate_Dura_Hemi_Resp
                             || SubjCode) +
                            (1 + Type_C + Dura_C + Hemi_C + 
                               Type_Dura + Type_Hemi + Dura_Hemi +
                               Type_Dura_Hemi +
                               Resp_C + 
                               Type_Resp + Dura_Resp + Hemi_Resp +
                               Type_Dura_Resp + Type_Hemi_Resp + Dura_Hemi_Resp +
                               Type_Hemi_Dura_Resp
                             || Stimuli),
                          data = df.erp.P1.E1,
                          REML = FALSE,
                          # verbose = TRUE,
                          control = lmerControl(optimizer = "bobyqa",  # nloptwrap Nelder_Mead
                                                optCtrl = list(maxfun = 1e7)))
} else {
  load(file.zcp.P1.E1)
}

print(summary(lmm.zcp.P1.E1), corr = FALSE)

```

```{r compare max and zcp E1 P1}
anova(lmm.max.P1.E1, lmm.zcp.P1.E1)
```

#### The reduced model
```{r PCA analysis for zcp lmm for E1 P1}
summary(rePCA(lmm.zcp.P1.E1))
```


```{r }

file.rdc.P1.E1 <- file.path(folder.nesi, "E204_P1_lmm_rdc.RData")

# fit the rdc model
if (!file.exists(file.rdc.P1.E1)) {
  # lmm.rdc.P1.E1.step <- step(lmm.rdc.P1.E1, reduce.fixed = FALSE)
  
  lmm.rdc.P1.E1 <- update(lmm.zcp.P1.E1,
                          formula = MeanAmp ~ Type * Category * Duration * Hemisphere * ACC +
                            (1 + Type_D + Dura_D + Hemi_D + 
                               Type_Dura + Cate_Dura + Type_Hemi + Dura_Hemi +
                               Type_Cate_Dura + Cate_Dura_Hemi + Type_Cate_ACC + Cate_Dura_ACC + Dura_Hemi_ACC +
                               Type_Cate_Dura_ACC || SubjCode),
                          verbose = FALSE)
  
} else {
  load(file.rdc.P1.E1)
}

print(summary(lmm.rdc.P1.E1), corr = FALSE)

```


```{r}
anova(lmm.rdc.P1.E1, lmm.zcp.P1.E1)
```


```{r}
summary(rePCA(lmm.rdc.P1.E1))
```


```{r}

file.rdc1.P1.E1 <- file.path(folder.nesi, "E204_P1_lmm_rdc1.RData")

# fit the rdc1 model
if (!file.exists(file.rdc1.P1.E1)) {
  # lmm.rdc1.P1.E1.step <- step(lmm.rdc1.P1.E1, reduce.fixed = FALSE)
  
  lmm.rdc1.P1.E1 <- update(lmm.rdc.P1.E1,
                          formula = MeanAmp ~ Type * Category * Duration * Hemisphere * ACC +
                            (1 + Type_D + Dura_D + Hemi_D + 
                               Type_Dura + Cate_Dura + Type_Hemi + Dura_Hemi +
                               Type_Cate_Dura + Cate_Dura_Hemi + Dura_Hemi_ACC +
                               Type_Cate_Dura_ACC || SubjCode),
                          verbose = FALSE)
  
} else {
  load(file.rdc1.P1.E1)
}

print(summary(lmm.rdc1.P1.E1), corr = FALSE)
```



### Ampitudes of the N170
```{r erp df for the N170 E1}
# only keep the data for amplitudes of the N170 (all erps)
df.erp.N170.E1 <- {
  df.erp.E1 %>% 
    filter(Component == "N170") %>% 
    substr_Event() 
}

if (saveCSV) {
  output.erp.N170.E1 <- file.path(folder.nesi, "E204_erp_N170.RData")
  save(df.erp.N170.E1, file = output.erp.N170.E1)
}

# the structure of this dataset
head(df.erp.N170.E1, 10)
```


#### The maximal model
```{r maximal lmm for E1 N170}
file.max.N170.E1 <- file.path(folder.nesi, "E204_N170_lmm_max.RData")

# fit the maximal model
if (!file.exists(file.max.N170.E1)) {
  lmm.max.N170.E1 <- lmer(MeanAmp ~ Type * Category * Duration * Hemisphere * Response + 
                            (1 + Type_C + Cate_C + Dura_C + Hemi_C +  
                               Type_Cate + Type_Dura + Cate_Dura + Type_Hemi + Cate_Hemi + Dura_Hemi +
                               Type_Cate_Dura + Type_Cate_Hemi + Type_Dura_Hemi + Cate_Dura_Hemi + 
                               Type_Cate_Dura_Hemi +
                               Resp_C + 
                               Type_Resp + Cate_Resp + Dura_Resp + Hemi_Resp +
                               Type_Cate_Resp + Type_Dura_Resp + Cate_Dura_Resp + 
                               Type_Hemi_Resp + Cate_Hemi_Resp + Dura_Hemi_Resp +
                               Type_Cate_Dura_Resp + Type_Cate_Hemi_Resp + Type_Hemi_Dura_Resp + Hemi_Cate_Dura_Resp +
                               Type_Cate_Dura_Hemi_Resp
                             | SubjCode) +
                            (1 + Type_C + Dura_C + Hemi_C + 
                               Type_Dura + Type_Hemi + Dura_Hemi +
                               Type_Dura_Hemi +
                               Resp_C + 
                               Type_Resp + Dura_Resp + Hemi_Resp +
                               Type_Dura_Resp + Type_Hemi_Resp + Dura_Hemi_Resp +
                               Type_Hemi_Dura_Resp
                             | Stimuli),
                          data = df.erp.N170.E1,
                          REML = FALSE,
                          # verbose = TRUE,
                          control = lmerControl(optimizer = "bobyqa",  # nloptwrap Nelder_Mead
                                                optCtrl = list(maxfun = 1e7)))
} else {
  load(file.max.N170.E1)
}

print(summary(lmm.max.N170.E1), corr = FALSE)

```


#### The zero-correlation-parameter model
```{r zcp lmm for E1 N170}

file.zcp.N170.E1 <- file.path(folder.nesi, "E204_N170_lmm_zcp.RData")

# fit the zcp model
if (!file.exists(file.zcp.N170.E1)) {
  lmm.zcp.N170.E1 <- lmer(MeanAmp ~ Type * Category * Duration * Hemisphere * urResponse + 
                            (1 + Type_C + Cate_C + Dura_C + Hemi_C +  
                               Type_Cate + Type_Dura + Cate_Dura + Type_Hemi + Cate_Hemi + Dura_Hemi +
                               Type_Cate_Dura + Type_Cate_Hemi + Type_Dura_Hemi + Cate_Dura_Hemi + 
                               Type_Cate_Dura_Hemi +
                               Resp_C + 
                               Type_Resp + Cate_Resp + Dura_Resp + Hemi_Resp +
                               Type_Cate_Resp + Type_Dura_Resp + Cate_Dura_Resp + 
                               Type_Hemi_Resp + Cate_Hemi_Resp + Dura_Hemi_Resp +
                               Type_Cate_Dura_Resp + Type_Cate_Hemi_Resp + Type_Hemi_Dura_Resp + Hemi_Cate_Dura_Resp +
                               Type_Cate_Dura_Hemi_Resp
                             || SubjCode) +
                            (1 + Type_C + Dura_C + Hemi_C + 
                               Type_Dura + Type_Hemi + Dura_Hemi +
                               Type_Dura_Hemi +
                               Resp_C + 
                               Type_Resp + Dura_Resp + Hemi_Resp +
                               Type_Dura_Resp + Type_Hemi_Resp + Dura_Hemi_Resp +
                               Type_Hemi_Dura_Resp
                             || Stimuli),
                          data = df.erp.N170.E1,
                          REML = FALSE,
                          # verbose = TRUE,
                          control = lmerControl(optimizer = "bobyqa",  # nloptwrap Nelder_Mead
                                                optCtrl = list(maxfun = 1e7)))
  
} else {
  load(file.zcp.N170.E1)
}


print(summary(lmm.zcp.N170.E1), corr = FALSE)

```


#### The reduced model
```{r PCA analysis for zcp lmm for E1 N170}
summary(rePCA(lmm.zcp.N170.E1))
```


```{r}

file.rdc.N170.E1 <- file.path(folder.nesi, "E204_N170_lmm_rdc.RData")

# fit the rdc model
if (!file.exists(file.rdc.N170.E1)) {
  # lmm.rdc.N170.E1.step <- step(lmm.rdc.N170.E1, reduce.fixed = FALSE)
  
  lmm.rdc.N170.E1 <- update(lmm.rdc.N170.E1,
                          formula = MeanAmp ~ Type * Category * Duration * Hemisphere * ACC +
                          (1 + Type_D + Cate_D + Dura_D + Hemi_D + 
                             Type_Cate + Type_Dura + Cate_Dura + Type_Hemi + Cate_Hemi + Dura_Hemi + Cate_ACC +
                             Type_Cate_Dura + Type_Cate_Hemi + Type_Dura_Hemi + Cate_Dura_ACC || SubjCode),
                          verbose = FALSE)
  
} else {
  load(file.rdc.N170.E1)
}

print(summary(lmm.rdc.N170.E1), corr = FALSE)

```

```{r}
anova(lmm.rdc.N170.E1, lmm.zcp.N170.E1)
```



```{r}

file.rdc1.N170.E1 <- file.path(folder.nesi, "E204_N170_lmm_rdc1.RData")

# fit the rdc1 model
if (!file.exists(file.rdc1.N170.E1)) {
  # lmm.rdc1.N170.E1.step <- step(lmm.rdc1.N170.E1, reduce.fixed = FALSE)
  
  lmm.rdc1.N170.E1 <- update(lmm.rdc1.N170.E1,
                          formula = MeanAmp ~ Type * Category * Duration * Hemisphere * ACC +
                          (1 + Type_D + Cate_D + Dura_D + Hemi_D + ACC_D +
                             Type_Cate + Type_Dura + Cate_Dura + Type_Hemi + Cate_Hemi + Dura_Hemi + Type_ACC + Cate_ACC + Dura_ACC + Hemi_ACC +
                             Type_Cate_Dura + Type_Cate_Hemi + Type_Dura_Hemi + Cate_Dura_Hemi + Type_Cate_ACC + Cate_Dura_ACC + Cate_Hemi_ACC || SubjCode),
                          verbose = FALSE)
  
} else {
  load(file.rdc1.N170.E1)
}

print(summary(lmm.rdc1.N170.E1), corr = FALSE)

```



# Experiment 2 (205)
To be tested in E2:
1. Differences for the N170 with difference condidences.
If 


## Behavior results
```{r load behavioral results of 205}
raw.beha.E2 <- {
  "E205_35_Behavior.csv" %>% 
    file.path(folder.data, .) %>% 
    read_csv() 
}

```


```{r select certain rows and columns from the data, and calculate the Z value for reaction times (205)}
clean.beha.E2 <- {
  raw.beha.E2 %>%
    filter(`Procedure[Trial]` == "expProc") %>% # remove rows for practice and countdown
    select(ExperimentName, SubjCode = Subject, Block, Trial, Age, Sex, Handedness, Type = `stimCategory[Trial]`, Category = `FH[Trial]`, Duration = `stimDuration[Trial]`, ACC = `Resp.ACC[Trial]`, Resp.RT = `Resp.RT[Trial]`, Resp = `Resp.RESP[Trial]`, Stimuli = `stimName[Trial]`) %>% # select and rename the columns (remove unuseful columns)
    mutate(
      SubjCode = factor(paste("P", SubjCode, sep = "")), # save SubjCode as factor
      Type = substr(Type, 1, 1),
      Type = recode_factor(Type, "N" = "intact", "S" = "scrambled"), # rename the levels of Type
      Category = recode(Category, "hosue" = "house", "house" = "house", "face" = "face", .default = "face"), # rename "hosue" as "house"
      Duration = factor(Duration), # save Duration as factor
      Category = as.factor(Category),
      Resp = as.factor(Resp)
      ) %>% 
    group_by(SubjCode, Type, Category, Duration, ACC) %>% # divide the data based on these conditions
    mutate(
      RT = Resp.RT + as.numeric(levels(Duration))[Duration],
      RT.Z = scale(RT), # calculate the Z value for reaction times within each group
      RT.Within3Z = ifelse(RT.Z <= 3 & RT.Z >= -3, 1, ifelse(RT.Z < -3 | RT.Z > 3, 0, NaN)) # if the Z values are between -3 and 3, will be marked as 1
      ) %>% 
    ungroup() # ungroup the data 
     
}

# behavior.tidyup
head(clean.beha.E2, 10)

```

```{r check the number of trials for 205}
# check the number of trials for 205
sum.count.E2 <- {
  clean.beha.E2 %>% 
    group_by(SubjCode, Type, Category, Duration) %>% 
    summarize(Count_sum = n())
}

valid.count.E2 <- {
  clean.beha.E2 %>% 
    filter(RT > RT.min) %>% 
    group_by(SubjCode, Type, Category, Duration) %>% 
    summarize(Count = n()) %>% 
    right_join(sum.count.E2, by = c("SubjCode", "Type", "Category", "Duration")) %>% 
    mutate(ratio = Count / Count_sum) %>% 
    filter(ratio > ratio.count) %$%
    SubjCode %>% 
    unique()
}

# remove the participants
clean.beha.E2 %<>% filter(SubjCode %in% valid.count.E2)

```


```{r remove participants whose performance in Key 1 was lower than 90%}
# remove participants whose performance in Key 1 was lower than 90%

valid.17 <- {
  clean.beha.E2 %>%
    filter(Type == "intact" & Duration == 17 & Resp == 1) %>% 
    group_by(SubjCode, Type, Duration, Resp) %>% 
    summarize(Accuracy = mean(ACC), Count = n()) %>% 
    filter(Accuracy > 0.95) %$%
    SubjCode
}

clean.beha.E2 %<>% filter(SubjCode %in% valid.17)

```

### Participant demographic information
```{r participant information}
subj.info.E2 <- {
  clean.beha.E2 %>% 
    select(SubjCode, Age, Sex, Handedness) %>% # select the columns of participant inforamtion
    distinct() # only keep the unique rows
}

subj.info.E2
```

### Response
```{r}
tmp.count <- {
  clean.beha.E2 %>% 
    group_by(SubjCode, Type, Category, Duration) %>% 
    summarize(Count_sum = n())
}


avg.resp.long.E2 <- {
  clean.beha.E2 %>% 
    # mutate(RespRateFace = if_else(Category == "face", ACC, if_else(Category == "house", as.numeric(!ACC), NaN))) %>% 
    group_by(SubjCode, Type, Category, Duration, Resp) %>% 
    summarize(Count = n()) %>% 
    
    right_join(tmp.count, by = c("SubjCode", "Type", "Category", "Duration")) %>% 
    mutate(RespRate = Count / Count_sum) %>% 
    ungroup()
}


# descriptive statistics of accuracy for plotting
desc.resp.E2 <- {
  avg.resp.long.E2 %>%
    group_by(Type, Category, Duration, Resp) %>%
    summarize(Mean = mean(RespRate)
              , N = n()
              , SD = if_else(N > 1, sd(RespRate), 0)
              , SE = SD/sqrt(N)
              , SE.lo = Mean - SE, SE.hi = Mean + SE
              , CI = SE * qt(0.975,N)
              , Median = median(RespRate)
              , Lower = Mean - SD, Upper = Mean + SD # for rainclound plot
              ) %>% 
    ungroup() %>% 
    add_row(Type = "intact", Category = "house", Duration = 200, Resp = 2, Mean = 0, N = 0, SD = 0, SE = 0, SE.lo = 0,
            SE.hi = 0, CI = 0, Median = 0, Lower = 0, Upper = 0)
}

avg.resp.wide.E2 <- {
  avg.resp.long.E2 %>% 
    mutate(Conditions = paste(Type, Category, Duration, Resp, sep = "_")) %>% 
    select(SubjCode, Conditions, RespRate) %>% 
    spread(Conditions, RespRate)
}


```


```{r rainclound plot of the RT data, warning = FALSE}
knitr::kable(desc.resp.E2, digits = 4)

rt.RainPlot.E2 <- {
  ggplot(data = avg.resp.long.E2, aes(y = RespRate, x = as.factor(Resp), fill = Category)) + 
    geom_flat_violin(position = position_nudge(x = .2, y = 0), alpha = .7) +
    geom_point(aes(y = RespRate, color = Category), position = position_jitter(width = .15), size = .5, alpha = .8) +
    geom_boxplot(aes(y = RespRate), width = .2, outlier.shape = NA, alpha = .7) +
    geom_point(data = desc.resp.E2, aes(y = Mean, x = Resp, color = Category), position = position_nudge(x = 0.3), size = 2.5) +
    geom_errorbar(data = desc.resp.E2, aes(y = Mean, ymin = Lower, ymax = Upper), position = position_nudge(x = 0.3), width = 0) +
    facet_grid(Type ~ Duration) +

    # facet_grid(. ~ Type, switch = "x") +  # create two panels to show data
    # scale_colour_grey() + # start = .1, end = .6, color for the contour
    # scale_fill_grey() + # start = .3, end = .6, color for the fill
    # scale_color_brewer(palette = "Set1") + # palette = "RdBu"
    # scale_fill_brewer(palette = "Set1") + # palette = "RdBu"
    labs(title = "Reaction times for E205", x = "Stimulus Type", y = "Reaction times (ms)", fill = "Duration(ms)", color = "Duration(ms)") +  # set the names for main, x and y axises and the legend
    # coord_cartesian(ylim = c(0, 1.05)) +  # set the limit for y axis
    # scale_y_continuous(expand= c(0, 0)) +  # remove the space between columns and x axis
    theme_bw() + # the backgroud color
    plot_theme
}

```

&nbsp;

```{r display the RainCloud plot for reaction times}
rt.RainPlot.E2
```



```{r}

acc.ColuPlot.E2 <- {
  ggplot(data = desc.resp.E2, aes(y = Mean, x = Resp, fill = Category)) +  # set the data, varialbes for x and y axises, and the variable for dividing data
    geom_col(position = "dodge", color = "black", alpha = .7) +  # position of columns and countour of columns
    facet_grid(Type ~ Duration) +
    geom_errorbar(mapping = aes(ymin = SE.lo, ymax = SE.hi), linetype = 1,  # set the error bar
                  show.legend = FALSE, width = 0.25, alpha = .5,
                  position = position_dodge(width=0.9)) +
    geom_hline(yintercept = c(1), linetype = 5, alpha = 0.5) +  # add the line for 0.5 and 1 (y)
    coord_cartesian(ylim = c(0,1.1)) +  # set the limit for y axis
    scale_y_continuous(expand= c(0, 0)) +  # remove the space between columns and x axis
    labs(title = "Subjective Responses for E205", x = "Responses", y = "Response Rate", fill = "Category") +  # set the names for main, x and y axises
    scale_fill_grey() +  # set the color for columns
    # geom_text(label = c("***", "***", "", ""), color = c("red", "red", "red", "red"), size = 6, nudge_y = 0.05, nudge_x = rep(c(-0.225, 0.225), 2)) + # add starts to the significant columns
    plot_theme
}

acc.ColuPlot.E2

```



## ERP amplitude analysis
### Load mean amplitude for single trials
The raw mean amplitude of each trial for all participants were loaded.
```{r load trial mean amplitude E2, message = FALSE}
# read the trial mean amplitude data
df.erp.E2 <- {
  "E205_MeanAmp.csv" %>% 
    file.path(folder.data, .) %>% 
    read_csv() %>% 
    substr_Event() %>% 
    mutate(SubjCode = as.factor(SubjCode),
           Hemisphere = as.factor(Hemisphere),
           urResponse = as.factor(urResponse),
           Stimuli = substr(stimName, 2, 6))
}

df.erp.E2 %>% 
  group_by(SubjCode, Stimuli, Hemisphere, Type, Category, Duration) %>% 
  summarize(count = n())

```

### Linear mixed model (all trials)

#### Set the successive difference coding
```{r set the dummy coding for ERP data E2}
# dummy coding
df.erp.E2.all <- sdif_coding_P203_erp(df.erp.E2)
```


#### Amplitudes of the P1
```{r erp df for the P1 E2}
# only keep the data for amplitudes of the P1 (correct and incorrect erps)
df.erp.P1.E2 <- {
  df.erp.E2.all %>% 
    filter(Component == "P1") %>% 
    substr_Event() 
}

if (saveCSV) {
  output.erp.P1.E2 <- file.path(folder.nesi, "E205_erp_P1.RData")
  save(df.erp.P1.E2, file = output.erp.P1.E2)
}

# the structure of this dataset
head(df.erp.P1.E2, 10)
```


##### The maximal model
```{r maximal lmm for P1 E2}
file.max.P1.E2 <- file.path(folder.nesi, "E205_P1_lmm_max.RData")

# fit the maximal model
if (!file.exists(file.max.P1.E2)) {
  lmm.max.P1.E2 <- lmer(MeanAmp ~ Type * Category * Duration * Hemisphere * Response + 
                          (1 + Type_C + Cate_C + Dura_C + Hemi_C +  
                             Type_Cate + Type_Dura + Cate_Dura + Type_Hemi + Cate_Hemi + Dura_Hemi +
                             Type_Cate_Dura + Type_Cate_Hemi + Type_Dura_Hemi + Cate_Dura_Hemi + 
                             Type_Cate_Dura_Hemi
                           | SubjCode) +
                          (1 + Type_C + Dura_C + Hemi_C + 
                             Type_Dura + Type_Hemi + Dura_Hemi +
                             Type_Dura_Hemi +
                             | Stimuli),
                        data = df.erp.P1.E2,
                        REML = FALSE,
                        # verbose = TRUE,
                        control = lmerControl(optimizer = "bobyqa",  # nloptwrap Nelder_Mead
                                              optCtrl = list(maxfun = 1e7)))
} else {
  load(file.max.P1.E2)
}

print(summary(lmm.max.P1.E2), corr = FALSE)

```

##### The zero-correlation-parameter model
```{r zcp lmm for P1 E2}
file.zcp.P1.E2 <- file.path(folder.nesi, "E205_P1_lmm_zcp.RData")

# fit the zcp model
if (!file.exists(file.zcp.P1.E2)) {
  lmm.zcp.P1.E2 <- lmer(MeanAmp ~ Type * Category * Duration * Hemisphere * Response + 
                            (1 + Type_C + Cate_C + Dura_C + Hemi_C +  
                               Type_Cate + Type_Dura + Cate_Dura + Type_Hemi + Cate_Hemi + Dura_Hemi +
                               Type_Cate_Dura + Type_Cate_Hemi + Type_Dura_Hemi + Cate_Dura_Hemi + 
                               Type_Cate_Dura_Hemi +
                               Resp_C + 
                               Type_Resp + Cate_Resp + Dura_Resp + Hemi_Resp +
                               Type_Cate_Resp + Type_Dura_Resp + Cate_Dura_Resp + 
                               Type_Hemi_Resp + Cate_Hemi_Resp + Dura_Hemi_Resp +
                               Type_Cate_Dura_Resp + Type_Cate_Hemi_Resp + Type_Hemi_Dura_Resp + Hemi_Cate_Dura_Resp +
                               Type_Cate_Dura_Hemi_Resp
                             || SubjCode) +
                            (1 + Type_C + Dura_C + Hemi_C + 
                               Type_Dura + Type_Hemi + Dura_Hemi +
                               Type_Dura_Hemi +
                               Resp_C + 
                               Type_Resp + Dura_Resp + Hemi_Resp +
                               Type_Dura_Resp + Type_Hemi_Resp + Dura_Hemi_Resp +
                               Type_Hemi_Dura_Resp
                             || Stimuli),
                          data = df.erp.P1.E2,
                          REML = FALSE,
                          # verbose = TRUE,
                          control = lmerControl(optimizer = "bobyqa",  # nloptwrap Nelder_Mead
                                                optCtrl = list(maxfun = 1e7)))
} else {
  load(file.zcp.P1.E2)
}

print(summary(lmm.zcp.P1.E2), corr = FALSE)

```

```{r compare max and zcp P1 E2}
anova(lmm.max.P1.E2, lmm.zcp.P1.E2)
```

##### The reduced model
```{r PCA analysis for zcp lmm for E2 P1}
summary(rePCA(lmm.zcp.P1.E2))
```


```{r }

file.rdc.P1.E2 <- file.path(folder.nesi, "E205_P1_lmm_rdc.RData")

# fit the rdc model
if (!file.exists(file.rdc.P1.E2)) {
  # lmm.rdc.P1.E2.step <- step(lmm.rdc.P1.E2, reduce.fixed = FALSE)
  
  lmm.rdc.P1.E2 <- update(lmm.zcp.P1.E2,
                          formula = MeanAmp ~ Type * Category * Duration * Hemisphere * ACC +
                            (1 + Type_D + Dura_D + Hemi_D + 
                               Type_Dura + Cate_Dura + Type_Hemi + Dura_Hemi +
                               Type_Cate_Dura + Cate_Dura_Hemi + Type_Cate_ACC + Cate_Dura_ACC + Dura_Hemi_ACC +
                               Type_Cate_Dura_ACC || SubjCode),
                          verbose = FALSE)
  
} else {
  load(file.rdc.P1.E2)
}

print(summary(lmm.rdc.P1.E2), corr = FALSE)

```



```{r}

file.rdc1.P1.E2 <- file.path(folder.nesi, "E205_P1_lmm_rdc1.RData")

# fit the rdc1 model
if (!file.exists(file.rdc1.P1.E2)) {
  # lmm.rdc1.P1.E2.step <- step(lmm.rdc1.P1.E2, reduce.fixed = FALSE)
  
  lmm.rdc1.P1.E2 <- update(lmm.rdc.P1.E2,
                          formula = MeanAmp ~ Type * Category * Duration * Hemisphere * ACC +
                            (1 + Type_D + Dura_D + Hemi_D + 
                               Type_Dura + Cate_Dura + Type_Hemi + Dura_Hemi +
                               Type_Cate_Dura + Cate_Dura_Hemi + Dura_Hemi_ACC +
                               Type_Cate_Dura_ACC || SubjCode),
                          verbose = FALSE)
  
} else {
  load(file.rdc1.P1.E2)
}

print(summary(lmm.rdc1.P1.E2), corr = FALSE)
```




#### Ampitudes of the N170
```{r erp df for the N170 E2}
# only keep the data for amplitudes of the N170 (all erps)
df.erp.N170.E2 <- {
  df.erp.E2.all %>% 
    filter(Component == "N170") %>% 
    substr_Event() 
}

if (saveCSV) {
  output.erp.N170.E2 <- file.path(folder.nesi, "E205_erp_N170.RData")
  save(df.erp.N170.E2, file = output.erp.N170.E2)
}

# the structure of this dataset
head(df.erp.N170.E2, 10)
```


##### The maximal model
```{r maximal lmm for E2 N170}
file.max.N170.E2 <- file.path(folder.nesi, "E205_N170_lmm_max.RData")

# fit the maximal model
if (!file.exists(file.max.N170.E2)) {
  lmm.max.N170.E2 <- lmer(MeanAmp ~ Type * Category * Duration * Hemisphere *  + 
                            (1 + Type_C + Cate_C + Dura_C + Hemi_C +  
                               Type_Cate + Type_Dura + Cate_Dura + Type_Hemi + Cate_Hemi + Dura_Hemi +
                               Type_Cate_Dura + Type_Cate_Hemi + Type_Dura_Hemi + Cate_Dura_Hemi + 
                               Type_Cate_Dura_Hemi +
                               | SubjCode) +
                            (1 + Type_C + Dura_C + Hemi_C + 
                               Type_Dura + Type_Hemi + Dura_Hemi +
                               Type_Dura_Hemi +
                               | Stimuli),
                          data = df.erp.N170.E2,
                          REML = FALSE,
                          # verbose = TRUE,
                          control = lmerControl(optimizer = "bobyqa",  # nloptwrap Nelder_Mead
                                                optCtrl = list(maxfun = 1e7)))
} else {
  load(file.max.N170.E2)
}

print(summary(lmm.max.N170.E2), corr = FALSE)

```


##### The zero-correlation-parameter model
```{r zcp lmm for E2 N170}

file.zcp.N170.E2 <- file.path(folder.nesi, "E205_N170_lmm_zcp.RData")

# fit the zcp model
if (!file.exists(file.zcp.N170.E2)) {
  lmm.zcp.N170.E2 <- lmer(MeanAmp ~ Type * Category * Duration * Hemisphere *  + 
                            (1 + Type_C + Cate_C + Dura_C + Hemi_C +  
                               Type_Cate + Type_Dura + Cate_Dura + Type_Hemi + Cate_Hemi + Dura_Hemi +
                               Type_Cate_Dura + Type_Cate_Hemi + Type_Dura_Hemi + Cate_Dura_Hemi + 
                               Type_Cate_Dura_Hemi +
                               Resp_C + 
                               Type_Resp + Cate_Resp + Dura_Resp + Hemi_Resp +
                               Type_Cate_Resp + Type_Dura_Resp + Cate_Dura_Resp + 
                               Type_Hemi_Resp + Cate_Hemi_Resp + Dura_Hemi_Resp +
                               Type_Cate_Dura_Resp + Type_Cate_Hemi_Resp + Type_Hemi_Dura_Resp + Hemi_Cate_Dura_Resp +
                               Type_Cate_Dura_Hemi_Resp
                             || SubjCode) +
                            (1 + Type_C + Dura_C + Hemi_C + 
                               Type_Dura + Type_Hemi + Dura_Hemi +
                               Type_Dura_Hemi +
                               Resp_C + 
                               Type_Resp + Dura_Resp + Hemi_Resp +
                               Type_Dura_Resp + Type_Hemi_Resp + Dura_Hemi_Resp +
                               Type_Hemi_Dura_Resp
                             || Stimuli),
                          data = df.erp.N170.E2,
                          REML = FALSE,
                          # verbose = TRUE,
                          control = lmerControl(optimizer = "bobyqa",  # nloptwrap Nelder_Mead
                                                optCtrl = list(maxfun = 1e7)))
  
} else {
  load(file.zcp.N170.E2)
}


print(summary(lmm.zcp.N170.E2), corr = FALSE)

```

```{r compare max and zcp E2 N170}
# anova(lmm.max.N170.E2, lmm.zcp.N170.E2)
```

##### The reduced model
```{r PCA analysis for zcp lmm for E2 N170}

summary(rePCA(lmm.zcp.N170.E2))
```


```{r}

file.rdc.N170.E2 <- file.path(folder.nesi, "E205_N170_lmm_rdc.RData")

# fit the rdc model
if (!file.exists(file.rdc.N170.E2)) {
  # lmm.rdc.N170.E2.step <- step(lmm.rdc.N170.E2, reduce.fixed = FALSE)
  
  lmm.rdc.N170.E2 <- update(lmm.rdc.N170.E2,
                          formula = MeanAmp ~ Type * Category * Duration * Hemisphere * ACC +
                          (1 + Type_D + Cate_D + Dura_D + Hemi_D + 
                             Type_Cate + Type_Dura + Cate_Dura + Type_Hemi + Cate_Hemi + Dura_Hemi + Cate_ACC +
                             Type_Cate_Dura + Type_Cate_Hemi + Type_Dura_Hemi + Cate_Dura_ACC || SubjCode),
                          verbose = FALSE)
  
} else {
  load(file.rdc.N170.E2)
}

print(summary(lmm.rdc.N170.E2), corr = FALSE)

```

```{r}
anova(lmm.rdc.N170.E2, lmm.zcp.N170.E2)
```



```{r}

file.rdc1.N170.E2 <- file.path(folder.nesi, "E205_N170_lmm_rdc1.RData")

# fit the rdc1 model
if (!file.exists(file.rdc1.N170.E2)) {
  # lmm.rdc1.N170.E2.step <- step(lmm.rdc1.N170.E2, reduce.fixed = FALSE)
  
  lmm.rdc1.N170.E2 <- update(lmm.rdc1.N170.E2,
                          formula = MeanAmp ~ Type * Category * Duration * Hemisphere * ACC +
                          (1 + Type_D + Cate_D + Dura_D + Hemi_D + ACC_D +
                             Type_Cate + Type_Dura + Cate_Dura + Type_Hemi + Cate_Hemi + Dura_Hemi + Type_ACC + Cate_ACC + Dura_ACC + Hemi_ACC +
                             Type_Cate_Dura + Type_Cate_Hemi + Type_Dura_Hemi + Cate_Dura_Hemi + Type_Cate_ACC + Cate_Dura_ACC + Cate_Hemi_ACC || SubjCode),
                          verbose = FALSE)
  
} else {
  load(file.rdc1.N170.E2)
}

print(summary(lmm.rdc1.N170.E2), corr = FALSE)

```


### Linear mixed model with subjective confidences

#### Converting response to subjective confidences
```{r}
df.erp.E2.SC <- {
  df.erp.E2 %>% 
    mutate(Confidence = if_else(Response %in% c("11", "55"), "high", 
                                if_else(Response %in% c("12", "54"), "low",
                                        if_else(substring(Response, 2) == "3", "guess", "NA")) )) %>% 
    filter(Confidence != "NA") %>% 
    mutate(Confidence = as.factor(Confidence))
}
```

#### Set the successive difference coding
```{r set the successive difference coding for ERP data E2 SC}
# successive difference coding
df.erp.E2.SC <- sdif_coding_E205_erp(df.erp.E2.SC)
```

#### Amplitudes of the P1
```{r erp df for the P1 E2 SC}
# only keep the data for amplitudes of the P1 (correct and incorrect erps)
df.erp.P1.E2.SC <- {
  df.erp.E2.SC %>% 
    filter(Component == "P1") %>% 
    substr_Event() 
}

if (saveCSV) {
  output.erp.P1.E2.SC <- file.path(folder.nesi, "E205_erp_P1_SC.RData")
  save(df.erp.P1.E2.SC, file = output.erp.P1.E2)
}

# the structure of this dataset
head(df.erp.P1.E2.SC, 10)
```


##### The maximal model
```{r maximal lmm for P1 E2 SC}
file.max.P1.E2.SC <- file.path(folder.nesi, "E205_P1_SC_lmm_max.RData")

# fit the maximal model
if (!file.exists(file.max.P1.E2.SC)) {
  lmm.max.P1.E2.SC <- lmer(MeanAmp ~ Type * Category * Duration * Hemisphere * Confidence + 
                             (1 + Type_C + Cate_C + Dura_C + Hemi_C +  
                                Type_Cate + Type_Dura + Cate_Dura + Type_Hemi + Cate_Hemi + Dura_Hemi +
                                Type_Cate_Dura + Type_Cate_Hemi + Type_Dura_Hemi + Cate_Dura_Hemi + 
                                Type_Cate_Dura_Hemi +
                                
                                ConLH_C + 
                                Type_ConLH + Cate_ConLH + Dura_ConLH + Hemi_ConLH +
                                Type_Cate_ConLH + Type_Dura_ConLH + Cate_Dura_ConLH + 
                                Type_Hemi_ConLH + Cate_Hemi_ConLH + Dura_Hemi_ConLH +
                                Type_Cate_Dura_ConLH + Type_Cate_Hemi_ConLH + Type_Hemi_Dura_ConLH + Hemi_Cate_Dura_ConLH +
                                Type_Cate_Dura_Hemi_ConLH +
                                
                                ConGL_C +
                                Type_ConGL + Cate_ConGL + Dura_ConGL + Hemi_ConGL +
                                Type_Cate_ConGL + Type_Dura_ConGL + Cate_Dura_ConGL + 
                                Type_Hemi_ConGL + Cate_Hemi_ConGL + Dura_Hemi_ConGL +
                                Type_Cate_Dura_ConGL + Type_Cate_Hemi_ConGL + Type_Hemi_Dura_ConGL + Hemi_Cate_Dura_ConGL +
                                Type_Cate_Dura_Hemi_ConGL 
                              | SubjCode) +
                             
                             (1 + Type_C + Dura_C + Hemi_C + 
                                Type_Dura + Type_Hemi + Dura_Hemi +
                                Type_Dura_Hemi +
                                
                                ConLH_C + 
                                Type_ConLH + Dura_ConLH + Hemi_ConLH +
                                Type_Dura_ConLH + Type_Hemi_ConLH + Dura_Hemi_ConLH +
                                Type_Hemi_Dura_ConLH +
                                
                                ConGL_C +
                                Type_ConGL + CDura_ConGL + Hemi_ConGL +
                                Type_Dura_ConGL + Type_Hemi_ConGL + Dura_Hemi_ConGL +
                                Type_Hemi_Dura_ConGL 
                              | Stimuli),
                           data = df.erp.P1.E2.SC,
                           REML = FALSE,
                           # verbose = TRUE,
                           control = lmerControl(optimizer = "bobyqa",  # nloptwrap Nelder_Mead
                                              optCtrl = list(maxfun = 1e7)))
} else {
  load(file.max.P1.E2.SC)
}

print(summary(lmm.max.P1.E2.SC), corr = FALSE)

```

##### The zero-correlation-parameter model
```{r zcp lmm for P1 E2 SC}
file.zcp.P1.E2.SC <- file.path(folder.nesi, "E205_P1_SC_lmm_zcp.RData")

# fit the zcp model
if (!file.exists(file.zcp.P1.E2.SC)) {
  lmm.zcp.P1.E2.SC <- lmer(MeanAmp ~ Type * Category * Duration * Hemisphere * Confidence + 
                             (1 + Type_C + Cate_C + Dura_C + Hemi_C +  
                                Type_Cate + Type_Dura + Cate_Dura + Type_Hemi + Cate_Hemi + Dura_Hemi +
                                Type_Cate_Dura + Type_Cate_Hemi + Type_Dura_Hemi + Cate_Dura_Hemi + 
                                Type_Cate_Dura_Hemi +
                                
                                ConLH_C + 
                                Type_ConLH + Cate_ConLH + Dura_ConLH + Hemi_ConLH +
                                Type_Cate_ConLH + Type_Dura_ConLH + Cate_Dura_ConLH + 
                                Type_Hemi_ConLH + Cate_Hemi_ConLH + Dura_Hemi_ConLH +
                                Type_Cate_Dura_ConLH + Type_Cate_Hemi_ConLH + Type_Hemi_Dura_ConLH + Hemi_Cate_Dura_ConLH +
                                Type_Cate_Dura_Hemi_ConLH +
                                
                                ConGL_C +
                                Type_ConGL + Cate_ConGL + Dura_ConGL + Hemi_ConGL +
                                Type_Cate_ConGL + Type_Dura_ConGL + Cate_Dura_ConGL + 
                                Type_Hemi_ConGL + Cate_Hemi_ConGL + Dura_Hemi_ConGL +
                                Type_Cate_Dura_ConGL + Type_Cate_Hemi_ConGL + Type_Hemi_Dura_ConGL + Hemi_Cate_Dura_ConGL +
                                Type_Cate_Dura_Hemi_ConGL 
                              || SubjCode) +
                             
                             (1 + Type_C + Dura_C + Hemi_C + 
                                Type_Dura + Type_Hemi + Dura_Hemi +
                                Type_Dura_Hemi +
                                
                                ConLH_C + 
                                Type_ConLH + Dura_ConLH + Hemi_ConLH +
                                Type_Dura_ConLH + Type_Hemi_ConLH + Dura_Hemi_ConLH +
                                Type_Hemi_Dura_ConLH +
                                
                                ConGL_C +
                                Type_ConGL + CDura_ConGL + Hemi_ConGL +
                                Type_Dura_ConGL + Type_Hemi_ConGL + Dura_Hemi_ConGL +
                                Type_Hemi_Dura_ConGL 
                              || Stimuli),
                           data = df.erp.P1.E2.SC,
                           REML = FALSE,
                           # verbose = TRUE,
                           control = lmerControl(optimizer = "bobyqa",  # nloptwrap Nelder_Mead
                                              optCtrl = list(maxfun = 1e7)))
} else {
  load(file.zcp.P1.E2.SC)
}

print(summary(lmm.zcp.P1.E2.SC), corr = FALSE)

```


##### The reduced model
```{r PCA analysis for zcp lmm for E2 P1 SC}
summary(rePCA(lmm.zcp.P1.E2.SC))
```


```{r }

file.rdc.P1.E2.SC <- file.path(folder.nesi, "E205_P1_SC_lmm_rdc.RData")

# fit the rdc model
if (!file.exists(file.rdc.P1.E2.SC)) {

  lmm.rdc.P1.E2.SC <- update(lmm.zcp.P1.E2.SC,
                          formula = MeanAmp ~ Type * Category * Duration * Hemisphere * ACC +
                            (1 + Type_D + Dura_D + Hemi_D + 
                               Type_Dura + Cate_Dura + Type_Hemi + Dura_Hemi +
                               Type_Cate_Dura + Cate_Dura_Hemi + Type_Cate_ACC + Cate_Dura_ACC + Dura_Hemi_ACC +
                               Type_Cate_Dura_ACC || SubjCode),
                          verbose = FALSE)
  
} else {
  load(file.rdc.P1.E2.SC)
}

print(summary(lmm.rdc.P1.E2.SC), corr = FALSE)

```



#### Ampitudes of the N170
```{r erp df for the N170 E2 SC}
# only keep the data for amplitudes of the N170 (all erps)
df.erp.N170.E2.SC <- {
  df.erp.E2.SC %>% 
    filter(Component == "N170") %>% 
    substr_Event() 
}

if (saveCSV) {
  output.erp.N170.E2.SC <- file.path(folder.nesi, "E205_erp_N170_SC.RData")
  save(df.erp.N170.E2.SC, file = output.erp.N170.E2.SC)
}

# the structure of this dataset
head(df.erp.N170.E2.SC, 10)
```


##### The maximal model
```{r maximal lmm for E2 N170 SC}
file.max.N170.E2.SC <- file.path(folder.nesi, "E205_N170_SC_lmm_max.RData")

# fit the maximal model
if (!file.exists(file.max.N170.E2.SC)) {
  lmm.max.N170.E2.SC <- lmer(MeanAmp ~ Type * Category * Duration * Hemisphere * Confidence + 
                             (1 + Type_C + Cate_C + Dura_C + Hemi_C +  
                                Type_Cate + Type_Dura + Cate_Dura + Type_Hemi + Cate_Hemi + Dura_Hemi +
                                Type_Cate_Dura + Type_Cate_Hemi + Type_Dura_Hemi + Cate_Dura_Hemi + 
                                Type_Cate_Dura_Hemi +
                                
                                ConLH_C + 
                                Type_ConLH + Cate_ConLH + Dura_ConLH + Hemi_ConLH +
                                Type_Cate_ConLH + Type_Dura_ConLH + Cate_Dura_ConLH + 
                                Type_Hemi_ConLH + Cate_Hemi_ConLH + Dura_Hemi_ConLH +
                                Type_Cate_Dura_ConLH + Type_Cate_Hemi_ConLH + Type_Hemi_Dura_ConLH + Hemi_Cate_Dura_ConLH +
                                Type_Cate_Dura_Hemi_ConLH +
                                
                                ConGL_C +
                                Type_ConGL + Cate_ConGL + Dura_ConGL + Hemi_ConGL +
                                Type_Cate_ConGL + Type_Dura_ConGL + Cate_Dura_ConGL + 
                                Type_Hemi_ConGL + Cate_Hemi_ConGL + Dura_Hemi_ConGL +
                                Type_Cate_Dura_ConGL + Type_Cate_Hemi_ConGL + Type_Hemi_Dura_ConGL + Hemi_Cate_Dura_ConGL +
                                Type_Cate_Dura_Hemi_ConGL 
                              | SubjCode) +
                             
                             (1 + Type_C + Dura_C + Hemi_C + 
                                Type_Dura + Type_Hemi + Dura_Hemi +
                                Type_Dura_Hemi +
                                
                                ConLH_C + 
                                Type_ConLH + Dura_ConLH + Hemi_ConLH +
                                Type_Dura_ConLH + Type_Hemi_ConLH + Dura_Hemi_ConLH +
                                Type_Hemi_Dura_ConLH +
                                
                                ConGL_C +
                                Type_ConGL + CDura_ConGL + Hemi_ConGL +
                                Type_Dura_ConGL + Type_Hemi_ConGL + Dura_Hemi_ConGL +
                                Type_Hemi_Dura_ConGL 
                              | Stimuli),
                          data = df.erp.N170.E2.SC,
                          REML = FALSE,
                          # verbose = TRUE,
                          control = lmerControl(optimizer = "bobyqa",  # nloptwrap Nelder_Mead
                                                optCtrl = list(maxfun = 1e7)))
} else {
  load(file.max.N170.E2.SC)
}

print(summary(lmm.max.N170.E2.SC), corr = FALSE)

```


##### The zero-correlation-parameter model
```{r zcp lmm for E2 N170 SC}

file.zcp.N170.E2.SC <- file.path(folder.nesi, "E205_N170_SC_lmm_zcp.RData")

# fit the zcp model
if (!file.exists(file.zcp.N170.E2.SC)) {
  lmm.zcp.N170.E2.SC <- lmer(MeanAmp ~ Type * Category * Duration * Hemisphere * Confidence + 
                             (1 + Type_C + Cate_C + Dura_C + Hemi_C +  
                                Type_Cate + Type_Dura + Cate_Dura + Type_Hemi + Cate_Hemi + Dura_Hemi +
                                Type_Cate_Dura + Type_Cate_Hemi + Type_Dura_Hemi + Cate_Dura_Hemi + 
                                Type_Cate_Dura_Hemi +
                                
                                ConLH_C + 
                                Type_ConLH + Cate_ConLH + Dura_ConLH + Hemi_ConLH +
                                Type_Cate_ConLH + Type_Dura_ConLH + Cate_Dura_ConLH + 
                                Type_Hemi_ConLH + Cate_Hemi_ConLH + Dura_Hemi_ConLH +
                                Type_Cate_Dura_ConLH + Type_Cate_Hemi_ConLH + Type_Hemi_Dura_ConLH + Hemi_Cate_Dura_ConLH +
                                Type_Cate_Dura_Hemi_ConLH +
                                
                                ConGL_C +
                                Type_ConGL + Cate_ConGL + Dura_ConGL + Hemi_ConGL +
                                Type_Cate_ConGL + Type_Dura_ConGL + Cate_Dura_ConGL + 
                                Type_Hemi_ConGL + Cate_Hemi_ConGL + Dura_Hemi_ConGL +
                                Type_Cate_Dura_ConGL + Type_Cate_Hemi_ConGL + Type_Hemi_Dura_ConGL + Hemi_Cate_Dura_ConGL +
                                Type_Cate_Dura_Hemi_ConGL 
                              || SubjCode) +
                             
                             (1 + Type_C + Dura_C + Hemi_C + 
                                Type_Dura + Type_Hemi + Dura_Hemi +
                                Type_Dura_Hemi +
                                
                                ConLH_C + 
                                Type_ConLH + Dura_ConLH + Hemi_ConLH +
                                Type_Dura_ConLH + Type_Hemi_ConLH + Dura_Hemi_ConLH +
                                Type_Hemi_Dura_ConLH +
                                
                                ConGL_C +
                                Type_ConGL + CDura_ConGL + Hemi_ConGL +
                                Type_Dura_ConGL + Type_Hemi_ConGL + Dura_Hemi_ConGL +
                                Type_Hemi_Dura_ConGL 
                              || Stimuli),
                          data = df.erp.N170.E2.SC,
                          REML = FALSE,
                          # verbose = TRUE,
                          control = lmerControl(optimizer = "bobyqa",  # nloptwrap Nelder_Mead
                                                optCtrl = list(maxfun = 1e7)))
  
} else {
  load(file.zcp.N170.E2.SC)
}


print(summary(lmm.zcp.N170.E2.SC), corr = FALSE)

```

```{r compare max and zcp E2 N170 SC}
# anova(lmm.max.N170.E2.SC, lmm.zcp.N170.E2.SC)
```

##### The reduced model
```{r PCA analysis for zcp lmm for E2 N170 SC}

summary(rePCA(lmm.zcp.N170.E2.SC))
```


```{r}

file.rdc.N170.E2.SC <- file.path(folder.nesi, "E205_N170_SC_lmm_rdc.RData")

# fit the rdc model
if (!file.exists(file.rdc.N170.E2.SC)) {

  lmm.rdc.N170.E2.SC <- update(lmm.rdc.N170.E2.SC,
                          formula = MeanAmp ~ Type * Category * Duration * Hemisphere * ACC +
                          (1 + Type_D + Cate_D + Dura_D + Hemi_D + 
                             Type_Cate + Type_Dura + Cate_Dura + Type_Hemi + Cate_Hemi + Dura_Hemi + Cate_ACC +
                             Type_Cate_Dura + Type_Cate_Hemi + Type_Dura_Hemi + Cate_Dura_ACC || SubjCode),
                          verbose = FALSE)
  
} else {
  load(file.rdc.N170.E2.SC)
}

print(summary(lmm.rdc.N170.E2.SC), corr = FALSE)

```

```{r}
anova(lmm.rdc.N170.E2.SC, lmm.zcp.N170.E2.SC)
```



















# Questions and predictions {#prediction}

** Predictions **



# Appendix

## Model selection procedure

For the analysis for every dependent variable, an optimal model was obtained by the following general steps:
  
1. **Maximum Model**: a maximum model with all fixed and random factors was built. 
2. **ZCP Model**: a zero-correlated-parameter (zcp) model based on the maximum model is built. The only difference between zcp and maximum models is that the correlations between random effects are forced to be zero in the zcp model.
3. **Reduced Model**: the random effects which are not supported by the data will be removed from the zcp model. The function `step(fit, fixed.reduce = FALSE)` from `library(lmerTest)` is used for this step. The algorithm could be found [here](#stepfun).
4. **Extended Model**: extending the reduced model with correlation parameters between the remaining random effects.
5. **Pruning Model**: pruning low correlation parameters. (Usually I don't do this.)
  
More details about this whole process chould be found here: Bates, D., Kliegl, R., Vasishth, S., & Baayen, H. (2015). Parsimonious Mixed Models. Retrieved from http://arxiv.org/abs/1506.04967.

## Algorithm of function `step` {#stepfun}
The outline of the algorithm of `step(fit, reduce.fixed = false)` function is: 

>  Simplification of the random effects structure:
>
>   1. Let M be the linear mixed effects model specified by a user. 
>
>   2. If there are random effects in M then go to 3, otherwise stop. 
>
>   3. For each random effect ri in M do: 
>       (a) Create a reduced model Mi by eliminating ri from M. 
>
>       (b) Calculate pi, the p value from the likelihood ratio test of comparing M to Mi. 
>
>       (c) Save pi and Mi.
>
>   4. Find pmax; the maximum of all pi and let Mmax denote the corresponding model. 
>
>   5. Set M to Mmax. If pmax is higher than  level then go back to 3, otherwise stop.

More deatils could be found: Kuznetsova, A., Brockhoff, P. B., & Christensen, R. H. B. (2017). lmerTest Package: Tests in Linear Mixed Effects Models. Journal of Statistical Software, 82(13), 1-26. http://doi.org/10.18637/jss.v082.i13 **Page 8**



# Versions of packages used
```{r versions}
rstudioapi::versionInfo()
sessionInfo()
```
