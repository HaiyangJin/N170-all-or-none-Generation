---
title: "P203 Subjective Perceptual and ERP Amplitude (N170)"
author: "[Haiyang Jin](https://haiyangjin.github.io/)"
date: "`r format(Sys.time(), '%d-%m-%Y')`"
output:
  html_document:
    df_print: paged
    number_sections: true
    toc: true
    toc_depth: 4
    toc_float: 
      collapsed: true
      smooth_scroll: false
    includes:
      after_body: Utilities/footer.html
---

# General introduction

## Introduction

**Linear Mixed Model** were used.
  
## Experiment design

* **Independent variables**:
    + Stimulus Type (`NS`: *normal* vs. *scrambled*)
    + Stimulus Category (`FH`: *face* vs. *house*)
    + Hemisphere (`Hemisphere`: *left* vs. *right*)
    + Durations (`Durations`: *17*, *50*, *100* vs. *200*)

* **Dependent variables**:
    + Gaussian -- *mean* (`MeanAmp`)
    
## Preparations
```{r, echo=FALSE, include=FALSE}
# knitr::opts_chunk$set(error = TRUE)
knitr::opts_chunk$set(echo = TRUE) # show the code for this chunk
options(width = 110)  # set the maximum width of the html output
```

```{r setup and load the related libraries, message=FALSE}
## load libraries
library(tidyverse)
library(magrittr)
library(lme4)
library(lmerTest) 
library(emmeans)

# options for calculating estimated marginal means
# lmer.df = c("kenward-roger", "satterthwaite", "asymptotic")
emm_options(lmer.df = "satterthwaite", lmerTest.limit = 1e6)  # , pbkrtest.limit =, disable.pbkrtest = FALSE

```
  
```{r foldes and load R files, message=, include=FALSE}
# folders
folder.R <- "R"
folder.data <- "data"
folder.nesi <- "NeSI"
folder.output <- "output"

# load R files
sapply(list.files(folder.R, "*.R", full.names = TRUE, recursive = TRUE), source)

# source(file.path(folder.R, "geom_flat_violin.R"))  # flat violin plot 
# source(file.path(folder.R, "substr_Event.R"))  # parse Event to IVs
# source(file.path(folder.R, "dummy_coding_P203.R")) # set the dummy coding manually
# source(file.path(folder.R, "get_pars.R")) #
# source(file.path(folder.R, "general_theme.R"))  # plot theme

```

```{r settings}
# general setting
saveCSV <- FALSE
ratio.count <- 0.8
RT.min <- 200

# set up the theme for plot and rainclound plot
plot_theme <- {
  general_theme +
    theme(legend.position = "right")
}

erp_theme <- {
  general_theme +
    theme(legend.position = "bottom")
}

ylimit.n170 <- c(2, -4)  # y axis for plotting N170 amplitude

```




# 204
## Behavioral results
```{r load behavioral results of 204}
raw.beha.E4 <- {
  "E204_Incorrect_Behavior.csv" %>% 
    file.path(folder.data, .) %>% 
    read_csv() 
}

```


```{r remove some trials for P426 and P428}
# locate the trials for P426 and P428
raw.beha.E4 <- {
  raw.beha.E4 %>%
    mutate(isBad426428 = ((Subject == 426) & Block == 6) | (Subject == 428 & Session == 2 & scrambleVar == "S"))  # remove some trials for 426 and 428. (Something wrong happened for the two participants during recording EEG, therefore they have two parts of behavioral data)
}

head(raw.beha.E4)
```

```{r select certain rows and columns from the data, and calculate the Z value for reaction times (204)}
clean.beha.E4 <- {
  raw.beha.E4 %>%
    filter(`Procedure[Trial]` %in% c("faceProc", "houseProc") & !isBad426428) %>% # select the rows (remove the practice trials and bad trials for 426 and 428)
    select(ExperimentName, SubjCode = Subject, Block, Trial, Age, Sex, Handedness, Type = scrambleVar, Category = FH, Duration = stimDuration, ACC = Resp.ACC, Resp.RT, Resp = Resp.RESP, Stimuli = stimName) %>% # select and rename the columns (remove unuseful columns)
    mutate(
      SubjCode = factor(paste("P", SubjCode, sep = "")), # save SubjCode as factor
      Type = recode_factor(Type, "N" = "normal", "S" = "scrambled"), # rename the levels of Type
      Category = recode(Category, "hosue" = "house", "house" = "house", "face" = "face", .default = "face"), # rename "hosue" as "house"
      Duration = factor(Duration) # save Duration as factor
      ) %>% 
    group_by(SubjCode, Type, Category, Duration, ACC) %>% # divide the data based on these conditions
    mutate(
      RT = Resp.RT + as.numeric(levels(Duration))[Duration],
      RT.Z = scale(RT), # calculate the Z value for reaction times within each group
      RT.Within3Z = ifelse(RT.Z <= 3 & RT.Z >= -3, 1, ifelse(RT.Z < -3 | RT.Z > 3, 0, NaN)) # if the Z values are between -3 and 3, will be marked as 1
      ) %>% 
    ungroup() # ungroup the data 
     
}

# behavior.tidyup
head(clean.beha.E4, 10)

```

```{r check the number of trials for 204}
# check the number of trials for 204
sum.count.E4 <- {
  clean.beha.E4 %>% 
    group_by(SubjCode, Type, Category, Duration) %>% 
    summarize(Count_sum = n())
}

valid.count.E4 <- {
  clean.beha.E4 %>% 
    filter(RT > RT.min) %>% 
    group_by(SubjCode, Type, Category, Duration) %>% 
    summarize(Count = n()) %>% 
    right_join(sum.count.E4, by = c("SubjCode", "Type", "Category", "Duration")) %>% 
    mutate(ratio = Count / Count_sum) %>% 
    filter(ratio > ratio.count) %$%
    SubjCode %>% 
    unique()
}


# remove the participants
clean.beha.E4 %<>% filter(SubjCode %in% valid.count.E4)

```


### Participant demographic information
```{r participant information}
subj.info.E4 <- {
  clean.beha.E4 %>% 
    select(SubjCode, Age, Sex, Handedness) %>% # select the columns of participant inforamtion
    distinct() # only keep the unique rows
}

subj.info.E4
```
There are `r nrow(subj.info.E4)` participants in total in this experiment.  \
The averaged age of this group is `r round(mean(subj.info.E4$Age), 3)` and the standard deviation of age is `r round(sd(subj.info.E4$Age))`. \
The number of female and male participants are `r sum(subj.info.E4$Sex == "female")` and `r sum(subj.info.E4$Sex == "male")` respectively. \
`r sum(subj.info.E4$Handedness == "right")` of them are right handed.

### Set the dummy coding
```{r dummy coding for acc E4}
# set the dummy coding manually 
clean.beha.E4 <- dummy_coding_P203(clean.beha.E4)

if (saveCSV) {
  output.beha.E4 <- file.path(folder.nesi, "E204_beha.RData")
  save(clean.beha.E4, file = output.beha.E4)
}
```

### Behavioral responses
#### Accuracy
```{r calculate the accuracy (E4)}
# accuracy data
sum.acc.R.E4 <- {
  clean.beha.E4 %>%
    group_by(SubjCode, Type, Duration) %>%
    summarize(Accuracy = mean(ACC)) %>%
    ungroup() 
}

# descriptive statistics of accuracy for plotting
desc.acc.R.E4 <- {
  sum.acc.R.E4 %>%
    group_by(Type, Duration) %>%
    summarize(Mean = mean(Accuracy)
              , N = n()
              , SD = sd(Accuracy)
              , SE = SD/sqrt(N)
              , SE.lo = Mean - SE, SE.hi = Mean + SE
              , CI = SE * qt(0.975,N)
              , Median = median(Accuracy)
              , Lower = Mean - SD, Upper = Mean + SD # for rainclound plot
              )  
}

# accuracy data for SPSS analysis
sum.acc.SPSS.E4 <- {
  sum.acc.R.E4 %>%
    mutate(Conditions = paste(Type, Duration, sep = ".")) %>%
    select(SubjCode, Conditions, Accuracy) %>%
    spread(Conditions, Accuracy)
}

# save the data as *.csv
if (saveCSV) {
  output.acc.SPSS.E4 <- file.path(folder.output, "E204_acc.csv")
  write.csv(sum.acc.SPSS.E4, output.acc.SPSS.E4, row.names = FALSE)
} 

```


##### RainClound plot of accuracy
```{r rainclound plot of accuracy E4, warning=FALSE}
# print(sum.acc.R, width = Inf)
knitr::kable(desc.acc.R.E4, digits = 4)

acc.RainPlot.E4 <- {
  ggplot(data = sum.acc.R.E4, aes(y = Accuracy, x = Type, fill = Duration)) + 
    geom_flat_violin(position = position_nudge(x = .2, y = 0), alpha = .7) +
    geom_point(aes(color = Duration), position = position_jitter(width = .15), size = .5, alpha = .8) +
    geom_boxplot(width = .2, outlier.shape = NA, alpha = .7) +
    geom_point(data = desc.acc.R.E4, aes(y = Mean, x = Type, color = Duration), position = position_nudge(x = 0.3), size = 2.5) +
    geom_errorbar(data = desc.acc.R.E4, aes(y = Mean, ymin = Lower, ymax = Upper), position = position_nudge(x = 0.3), width = 0) +
    # scale_colour_grey() + # start = .1, end = .6, color for the contour
    # scale_fill_grey() + # start = .3, end = .6, color for the fill
    # scale_color_brewer(palette = "Set1") +
    # scale_fill_brewer(palette = "Set1") +
    labs(title = "Accuracy for E204", x = "Stimulus Type", y = "Accuracy", fill = "Duration(ms)", color = "Duration(ms)") +  # set the names for main, x and y axises and the legend
    coord_cartesian(ylim = c(0, 1.05)) +  # set the limit for y axis
    scale_y_continuous(expand= c(0, 0)) +  # remove the space between columns and x axis
    geom_hline(yintercept = c(0.5, 1), linetype = 5, alpha = 0.5) + # add the line for 0.5 and 1 (y)
    theme_bw() + # the backgroud color
    plot_theme
}

```

&nbsp;

```{r display the RainCloud plot of accuracy}
acc.RainPlot.E4
```


##### Column plot of accuracy
```{r Plot of accuracy of behavioral data E4, warning = FALSE}

acc.ColuPlot.E4 = {
  ggplot(data = desc.acc.R.E4, aes(y = Mean, x = Type, fill = Duration )) +  # set the data, varialbes for x and y axises, and the variable for dividing data
    geom_col(position = "dodge", color = "black", alpha = .7) +  # position of columns and countour of columns
    geom_errorbar(mapping = aes(ymin = SE.lo, ymax = SE.hi), linetype = 1,  # set the error bar
                  show.legend = FALSE, width = 0.25, alpha = .5, 
                  position = position_dodge(width=0.9)) +
    geom_hline(yintercept = c(0.5, 1), linetype = 5, alpha = 0.5) +  # add the line for 0.5 and 1 (y)
    coord_cartesian(ylim = c(0,1.1)) +  # set the limit for y axis
    scale_y_continuous(expand= c(0, 0)) +  # remove the space between columns and x axis
    labs(title = "Accuracy for E204", x = "Stimulus Type", y = "Accuracy", fill = "Durations(ms)") +  # set the names for main, x and y axises
    scale_fill_grey() +  # set the color for columns
    geom_text(label = c("***", "***", "", ""), color = c("red", "red", "red", "red"), size = 6, nudge_y = 0.05, nudge_x = rep(c(-0.225, 0.225), 2)) + # add starts to the significant columns
    theme_bw() +
    plot_theme
}

acc.ColuPlot.E4
```

#### Hits
```{r calculate the hits (and save the data into csv) E4}

# hit data for R analysis
sum.hit.R.E4 <- {
  clean.beha.E4 %>%
    group_by(SubjCode, Type, Category, Duration) %>%
    summarize(Hit = mean(ACC), count = n()) %>%
    ungroup()
}

# descriptive statistics of hits for plotting
desc.hit.R.E4 <- {
  sum.hit.R.E4 %>% 
    group_by(Type, Category, Duration) %>% 
    summarize(Mean = mean(Hit), N = n(), SD = sd(Hit), SE = SD/sqrt(N), SE.hi = Mean + SE, SE.lo = Mean - SE, CI = SE * qt(0.975,N), Median = median(Hit), Lower = Mean - SD, Upper = Mean + SD)
}

# hit data for SPSS analysis
sum.hit.SPSS.E4 <- {
  sum.hit.R.E4 %>%
    mutate(Conditions = paste(Type, Category, Duration, sep = ".")) %>%
    select(SubjCode, Conditions, Hit) %>%
    spread(Conditions, Hit)
}

# save the data as *.csv
if (saveCSV) {
  output.hit.SPSS.E4 <- file.path(folder.output, "E204_hit.csv")
  write.csv(sum.hit.SPSS.E4, output.hit.SPSS.E4, row.names = FALSE)
} 

```


##### RainClound plot of hits
```{r rainclound plot of hits E4}
knitr::kable(desc.hit.R.E4, digits = 4)

hit.RainPlot.E4 <- {
  ggplot(data = sum.hit.R.E4, aes(y = Hit, x = Category, fill = Duration)) + 
    geom_flat_violin(position = position_nudge(x = .2, y = 0), alpha = .7) +
    geom_point(aes(y = Hit, color = Duration), position = position_jitter(width = .15), size = .5, alpha = .8) +
    geom_boxplot(aes(y = Hit), width = .2, outlier.shape = NA, alpha = .7) +
    geom_point(data = desc.hit.R.E4, aes(y = Mean, x = Category, color = Duration), position = position_nudge(x = 0.3), size = 2.5) +
    geom_errorbar(data = desc.hit.R.E4, aes(y = Mean, ymin = Lower, ymax = Upper), position = position_nudge(x = 0.3), width = 0) +
    facet_grid(. ~ Type, switch = "x") +  # create two panels to show data
    # scale_colour_grey() + # start = .1, end = .6, color for the contour
    # scale_fill_grey() + # start = .3, end = .6, color for the fill
    # scale_color_brewer(palette = "Set1") + # palette = "RdBu"
    # scale_fill_brewer(palette = "Set1") + # palette = "RdBu"
    labs(title = "Hits for E204", x = "Stimulus Type", y = "Hit Rate", fill = "Duration(ms)", color = "Duration(ms)") +  # set the names for main, x and y axises and the legend
    coord_cartesian(ylim = c(0, 1.05)) +  # set the limit for y axis
    scale_y_continuous(expand= c(0, 0)) +  # remove the space between columns and x axis
    geom_hline(yintercept = c(0.5, 1), linetype = 5, alpha = 0.5) + # add the line for 0.5 and 1 (y)
    theme_bw() + # the backgroud color
    plot_theme
}

```

&nbsp;

```{r display the RainCloud plot of hits E4}
hit.RainPlot.E4
```

##### Column plot of hits
```{r Plot of hits for behavioral data E4, warning=FALSE}

hit.ColuPlot.E4 = {
  ggplot(data = desc.hit.R.E4, aes(y = Mean, x = Category, fill = Duration )) +  # set the data, varialbes for x and y axises, and the variable for dividing data
    geom_col(position = "dodge", color = "black", alpha = .7) +  # position of columns and countour of columns
    geom_errorbar(mapping = aes(ymin = SE.lo, ymax = SE.hi), linetype = 1,  # set the error bar
                  show.legend = FALSE, width = 0.25, alpha = .5, 
                  position = position_dodge(width=0.9)) +
    facet_grid(. ~ Type, switch = "x") +  # create two panels to show data
    geom_hline(yintercept = c(0.5, 1), linetype = 5, alpha = 0.5) +  # add the line for 0.5 and 1 (y)
    coord_cartesian(ylim = c(0,1.1)) +  # set the limit for y axis
    scale_y_continuous(expand= c(0, 0)) +  # remove the space between columns and x axis
    labs(title = "Hits for E204", x = "Stimulus Type", y = "Hit Rate", fill = "Durations(ms)") +  # set the names for main, x and y axises
    scale_fill_grey() +  # set the color for columns
    # scale_fill_manual(values=c(faceColors, houseColors)) +  # set the color for columns and the label names for legend  , name = "xxx"
    geom_text(label = c("***", "***", "***", "***", "*", "", "", ""), color = rep("red", 8), size = 6, nudge_y = 0.05, nudge_x = rep(c(-0.225, 0.225), 4)) + # add starts to the significant columns
    theme_bw() +
    plot_theme
}

hit.ColuPlot.E4
```


#### Logit mixed model
##### The maximal model
```{r maximal lmm for E204 Accuracy}
file.max.acc.E4 <- file.path(folder.nesi, "E204_acc_glmm_max.RData")

if (!file.exists(file.max.acc.E4)) {
  glmm.max.acc.E4 <- glmer(ACC ~ Type * Category * Duration +
                             (1 + Type_D + Cate_D + Dura_D + Type_Cate + Type_Dura + Cate_Dura + Type_Cate_Dura | SubjCode) +
                             (1 + Dura_D | Stimuli),
                           data = clean.beha.E4,
                           family = "binomial",
                           control = glmerControl(optCtrl = list(maxfun = 1e6)))
} else {
  load(file.max.acc.E4)
}

print(summary(glmm.max.acc.E4), corr = FALSE)

```


##### The zero-correlation-parameter model
```{r zcp lmm for E204 Accuracy}
file.zcp.acc.E4 <- file.path(folder.nesi, "E204_acc_glmm_zcp.RData")

if (!file.exists(file.zcp.acc.E4)) {
  glmm.zcp.acc.E4 <- update(glmm.max.acc.E4,
                           formula = ACC ~ Type * Category * Duration + 
                            (1 + Type_D + Cate_D + Dura_D + Type_Cate + Type_Dura + Cate_Dura + Type_Cate_Dura || SubjCode) +
                            (1 + Dura_D || Stimuli),
                          verbose = FALSE)
  
  glmm.zcp.acc.E4.pars <- get_pars(glmm.zcp.acc.E4)
  
  glmm.zcp1.acc.E4 <- update(glmm.zcp.acc.E4,
                             start = glmm.zcp.acc.E4.pars)
  
} else {
  load(file.zcp.acc.E4)
}

print(summary(glmm.zcp.acc.E4), corr = FALSE)

```

```{r}
print(summary(glmm.zcp1.acc.E4), corr = FALSE)

```


##### The reduced model
```{r}
summary(rePCA(glmm.zcp1.acc.E4))

```

```{r rdc lmm for E204 Accuracy}

# file.rdc.acc.E4 <- file.path(folder.nesi, "E204_acc_glmm_rdc.RData")
# 
# if (!file.exists(file.rdc.acc.E4)) {
#   glmm.rdc.acc.E4 <- update(glmm.zcp.acc.E4,
#                            formula = ACC ~ Type * Category * Duration + 
#                             (1 + Type_D + Cate_D + Dura_D + Type_Cate + Type_Dura + Cate_Dura + Type_Cate_Dura || SubjCode) +
#                             (1 + Dura_D || Stimuli),
#                           verbose = FALSE)
# } else {
#   load(file.rdc.acc.E4)
# }
# 
# print(summary(glmm.rdc.acc.E4), corr = FALSE)

```


```{r}

# glmm.rdc.acc.E4.pars <- get_pars(glmm.rdc.acc.E4)
# glmm.rdc1.acc.E4 <- update(glmm.rdc.acc.E4, start = glmm.rdc.acc.E4.pars)
# print(summary(glmm.rdc1.acc.E4), corr = FALSE)

```


##### The extended model
```{r etd lmm for E204 Accuracy}

# file.etd.acc.E4 <- file.path(folder.nesi, "E204_acc_glmm_etd.RData")
# 
# if (!file.exists(file.etd.acc.E4)) {
#   glmm.etd.acc.E4 <- update(glmm.rdc.acc.E4,
#                            formula = ACC ~ Type * Category * Duration + 
#                             (1 + Type_D + Cate_D + Dura_D + Type_Cate + Type_Dura + Cate_Dura + Type_Cate_Dura | SubjCode) +
#                             (1 + Dura_D | Stimuli),
#                           verbose = FALSE)
# } else {
#   load(file.etd.acc.E4)
# }
# 
# print(summary(glmm.etd.acc.E4), corr = FALSE)

```

```{r}
# pars.etd.acc.E4 <- get_pars(glmm.etd.acc.E4)
# glmm.etd1.acc.E4 <- update(glmm.etd.acc.E4, start = pars.etd.acc.E4)
# 
# print(summary(glmm.etd1.acc.E4), corr = FALSE)
```


### Correct response times
```{r calculate the correct RT E4}
df.rt.E4 <- {
  clean.beha.E4 %>%
    filter((ACC == 1 | Type == "scrambled")
           & RT.Within3Z == 1 # remove the RT outsides 3Z
           )
}

sum.rt.R.E4 <- {
   df.rt.E4 %>% # only include the correct responses for normal trials but all trial for scrambled
    select(SubjCode, Type, Category, Duration, RT) %>% # select the columns
    group_by(SubjCode, Type, Category, Duration) %>% # divide data into groups to get the mean
    summarize(RT = mean(RT), count = n()) %>%  # calculate the mean of RT
    ungroup()
}

desc.rt.R.E4 <- {
  sum.rt.R.E4 %>% 
    group_by(Type, Category, Duration) %>% 
    summarize(Mean = mean(RT), N = n(), SD = sd(RT), SE = SD/sqrt(N), SE.hi = Mean + SE, SE.lo = Mean - SE, CI = SE * qt(0.975,N), Median = median(RT), Lower = Mean - SD, Upper = Mean + SD)
}

sum.rt.SPSS.E4 <- {
  sum.rt.R.E4 %>% 
    mutate(Conditions = paste(Type, Category, Duration, sep = ".")) %>%
    select(-c(Type, Category, Duration, count)) %>%
    spread(Conditions, RT)
}

 # save the data into *.csv
if (saveCSV) {
  output.rt.E4 <- file.path(folder.nesi, "E204_beha_RT.RData")
  save(df.rt.E4, file = output.rt.E4)
  output.rt.SPSS.E4 <- file.path(folder.output, "E204_RT.csv")
  write.csv(sum.rt.SPSS.E4, output.rt.SPSS.E4, row.names = FALSE)
}
```

#### RainClound plot of the response times 
```{r rainclound plot of the RT data, warning = FALSE}
knitr::kable(desc.rt.R.E4, digits = 4)

rt.RainPlot.E4 <- {
  ggplot(data = sum.rt.R.E4, aes(y = RT, x = Category, fill = Duration)) + 
    geom_flat_violin(position = position_nudge(x = .2, y = 0), alpha = .7) +
    geom_point(aes(y = RT, color = Duration), position = position_jitter(width = .15), size = .5, alpha = .8) +
    geom_boxplot(aes(y = RT), width = .2, outlier.shape = NA, alpha = .7) +
    geom_point(data = desc.rt.R.E4, aes(y = Mean, x = Category, color = Duration), position = position_nudge(x = 0.3), size = 2.5) +
    geom_errorbar(data = desc.rt.R.E4, aes(y = Mean, ymin = Lower, ymax = Upper), position = position_nudge(x = 0.3), width = 0) +
    facet_grid(. ~ Type, switch = "x") +  # create two panels to show data
    # scale_colour_grey() + # start = .1, end = .6, color for the contour
    # scale_fill_grey() + # start = .3, end = .6, color for the fill
    # scale_color_brewer(palette = "Set1") + # palette = "RdBu"
    # scale_fill_brewer(palette = "Set1") + # palette = "RdBu"
    labs(title = "Reaction times for E204", x = "Stimulus Type", y = "Reaction times (ms)", fill = "Duration(ms)", color = "Duration(ms)") +  # set the names for main, x and y axises and the legend
    # coord_cartesian(ylim = c(0, 1.05)) +  # set the limit for y axis
    # scale_y_continuous(expand= c(0, 0)) +  # remove the space between columns and x axis
    theme_bw() + # the backgroud color
    plot_theme
}

```

&nbsp;

```{r display the RainCloud plot for reaction times}
rt.RainPlot.E4
```


#### Column plot of the response times
```{r Plot of RT data}

rt.ColuPlot.E4 = {
  ggplot(data = desc.rt.R.E4, aes(y = Mean, x = Category, fill = Duration )) +  # set the data, varialbes for x and y axises, and the variable for dividing data
    geom_col(position = "dodge", color = "black", alpha = .7) +  # position of columns and countour of columns
    geom_errorbar(mapping = aes(ymin = SE.lo, ymax = SE.hi), linetype = 1,  # set the error bar
                  show.legend = FALSE, width = 0.25, alpha = .5, 
                  position = position_dodge(width=0.9)) +
    facet_grid(. ~ Type, switch = "x") +  # create two panels to show data
    coord_cartesian(ylim = c(0, 850)) +  # set the limit for y axis
    scale_y_continuous(expand= c(0, 0)) +  # remove the space between columns and x axis
    labs(title = "Reaction times for E204", x = "Stimulus Type", y = "Reaction times (ms)", fill = "Durations(ms)") +  # set the names for main, x and y axises
    scale_fill_grey() +  # set the color for columns
    # geom_text(label = c("*", "***", "", ""), color = c("red", "red", "red", "red"), size = 6, nudge_y = 0.05, nudge_x = rep(c(-0.225, 0.225), 2)) + # add starts to the significant columns
    theme_bw() +
    plot_theme
}

rt.ColuPlot.E4
```

#### Linear mixed model
##### The maximal model
```{r maximal lmm for E204 RT}
file.max.rt.E4 <- file.path(folder.nesi, "E204_rt_lmm_max.RData")

if (!file.exists(file.max.rt.E4)) {
  lmm.max.rt.E4 <- lmer(RT ~ Type * Category * Duration +
                          (1 + Type_D + Cate_D + Dura_D + Type_Cate + Type_Dura + Cate_Dura + Type_Cate_Dura | SubjCode) +
                          (1 + Dura_D | Stimuli),
                        data = df.rt.E4,
                        verbose = TRUE,
                        REML = FALSE,
                        control = lmerControl(optimizer = "bobyqa",
                                              optCtrl = list(maxfun = 1e5)))
} else {
  load(file.max.rt.E4)
}

print(summary(lmm.max.rt.E4), corr = FALSE)

```


##### The zero-correlation-parameter model
```{r zcp lmm for E204 RT}
file.zcp.rt.E4 <- file.path(folder.nesi, "E204_rt_lmm_zcp.RData")

if (!file.exists(file.zcp.rt.E4)) {
  lmm.zcp.rt.E4 <- update(lmm.max.rt.E4,
                          formula = RT ~ Type * Category * Duration +
                          (1 + Type_D + Cate_D + Dura_D + Type_Cate + Type_Dura + Cate_Dura + Type_Cate_Dura || SubjCode) +
                          (1 + Dura_D || Stimuli),
                          verbose = FALSE
                          )
} else {
  load(file.zcp.rt.E4)
}

print(summary(lmm.zcp.rt.E4), corr = FALSE)

```


##### The reduced model
```{r}
summary(rePCA(lmm.zcp.rt.E4))
```


```{r reduced lmm for E204 RT}
file.rdc.rt.E4 <- file.path(folder.nesi, "E204_rt_lmm_rdc.RData")

if (!file.exists(file.rdc.rt.E4)) {
  lmm.rdc.rt.E4 <- update(lmm.max.rt.E4,
                          formula = RT ~ Type * Category * Duration +
                          (1 + Type_D + Cate_D + Dura_D + Type_Cate + Type_Dura + Cate_Dura + Type_Cate_Dura || SubjCode) +
                          (1 | Stimuli),
                          verbose = FALSE
                          )
} else {
  load(file.rdc.rt.E4)
}

print(summary(lmm.rdc.rt.E4), corr = FALSE)

```

##### The extended model
```{r extended lmm for E204 RT}
file.etd.rt.E4 <- file.path(folder.nesi, "E204_rt_lmm_etd.RData")

if (!file.exists(file.etd.rt.E4)) {
  lmm.etd.rt.E4 <- update(lmm.rdc.rt.E4,
                          formula = RT ~ Type * Category * Duration +
                          (1 + Type_D + Cate_D + Dura_D + Type_Cate + Type_Dura + Cate_Dura + Type_Cate_Dura | SubjCode) +
                          (1 | Stimuli),
                          verbose = FALSE
                          )
} else {
  load(file.etd.rt.E4)
}

print(summary(lmm.etd.rt.E4), corr = FALSE)

```


```{r}
summary(rePCA(lmm.etd.rt.E4))
```

```{r extended1 lmm for E204 RT}
file.etd1.rt.E4 <- file.path(folder.nesi, "E204_rt_lmm_etd1.RData")

if (!file.exists(file.etd1.rt.E4)) {
  lmm.etd1.rt.E4 <- update(lmm.etd.rt.E4,
                          formula = RT ~ Type * Category * Duration +
                          (1 + Type_D + Dura_D + Type_Dura + Type_Cate_Dura | SubjCode) +
                          (1 | Stimuli),
                          verbose = FALSE
                          )
} else {
  load(file.etd1.rt.E4)
}

print(summary(lmm.etd1.rt.E4), corr = FALSE)

```


```{r}
anova(lmm.etd1.rt.E4, lmm.rdc.rt.E4)
```

##### The optimal model
```{r}
lmm.opt.rt.E4 <- update(lmm.rdc.rt.E4,
                        REML = TRUE,
                        verbose = FALSE)

summary(lmm.opt.rt.E4)
```

##### Disgnostic plots
```{r qqplot for lmm.opt.rt.E4}
# qqplot
qqplot_lmer(lmm.opt.rt.E4)
```


#### Log mixed model
##### The maximal model
```{r maximal glmm for E204 RT}
file.max.rt.E4 <- file.path(folder.nesi, "E204_rt_glmm_max.RData")

if (!file.exists(file.max.rt.E4)) {
  glmm.max.rt.E4 <- glmer(RT ~ Type * Category * Duration +
                             (1 + Type_D + Cate_D + Dura_D + Type_Cate + Type_Dura + Cate_Dura + Type_Cate_Dura | SubjCode) +
                             (1 + Dura_D | Stimuli),
                           data = df.rt.E4,
                           family = "poisson",
                           control = glmerControl(optCtrl = list(maxfun = 1e5)))
} else {
  load(file.max.rt.E4)
}

print(summary(glmm.max.rt.E4), corr = FALSE)

```

##### The zero-correlation-parameter model
```{r zcp glmm for E204 RT}
file.zcp.rt.E4 <- file.path(folder.nesi, "E204_rt_glmm_zcp.RData")

# fit the zcp model
if (!file.exists(file.zcp.rt.E4)) {
  glmm.zcp.rt.E4 <- update(glmm.max.rt.E4, 
                          formula = RT ~ Type * Category * Duration +
                             (1 + Type_D + Cate_D + Dura_D + Type_Cate + Type_Dura + Cate_Dura + Type_Cate_Dura || SubjCode) +
                             (1 + Dura_D || Stimuli),
                          verbose = FALSE)
} else {
  load(file.zcp.rt.E4)
}

print(summary(glmm.zcp.rt.E4), corr = FALSE)

```

```{r compare max and zcp E204 rt}
anova(glmm.max.rt.E4, glmm.zcp.rt.E4)
```

##### The reduced model
```{r PCA for zcp RT 204}
# PCA
summary(rePCA(glmm.zcp.rt.E4))
```


```{r reduced glmm for E204 RT}

# file.rdc.rt.E4 <- file.path(folder.nesi, "E204_rt_glmm_rdc.RData")
# 
# # fit the reduced model
# if (!file.exists(file.rdc.rt.E4)) {
#   glmm.rdc.rt.E4 <- update(glmm.zcp.rt.E4,
#                            formula = RT ~ Type * Category * Duration +
#                              (1 + Type_D + Cate_D + Dura_D + Type_Cate + Type_Dura + Cate_Dura + Type_Cate_Dura || SubjCode) +
#                              (1 + Type_D + Dura_D + Type_Dura + Type_Cate_Dura || Stimuli))
# } else {
#   load(file.rdc.rt.E4)
# }
# 
# print(summary(glmm.rdc.rt.E4), corr = FALSE)
```

```{r}
anova(glmm.rdc.rt.E4, glmm.zcp.rt.E4)
```


##### The extended model



## ERP data analysis
### Load mean amplitude for single trials
The raw mean amplitude of each trial for all participants were loaded.
```{r load trial mean amplitude, message = FALSE}
# read the trial mean amplitude data
df.erp.E4 <- {
  "P203_204_RawSTData.csv" %>% 
    file.path(folder.data, .) %>% 
    read_csv() %>% 
    substr_Event() %>% 
    mutate(ExpCode = as.factor(ExpCode),
           SubjCode = as.factor(SubjCode),
           Hemisphere = as.factor(Hemisphere))
}

```

### Set the dummy coding
```{r set the dummy coding for ERP data E4}
# dummy coding
df.erp.E4 <- dummy_coding_P203_erp(df.erp.E4)
```


### Amplitudes of the P1
```{r erp df for the P1}
# only keep the data for amplitudes of the P1 (correct and incorrect erps)
df.erp.P1.E4 <- {
  df.erp.E4 %>% 
    filter(Component == "P1", ACC != "all") %>% 
    substr_Event() 
}

if (saveCSV) {
  output.erp.P1.E4 <- file.path(folder.nesi, "E204_erp_P1.RData")
  save(df.erp.P1.E4, file = output.erp.P1.E4)
}

# the structure of this dataset
head(df.erp.P1.E4, 10)
```


#### Build up 
```{r}
lmm.P1.E4.1 <- lmer(MeanAmp ~ Type * Category * Duration * ACC + 
                      (1 + Type | SubjCode),
                    data = df.erp.P1.E4,
                    REML = FALSE,
                    control = lmerControl(optimizer = "bobyqa")) 

print(summary(lmm.P1.E4.1), corr = FALSE)
```

```{r}
lmm.P1.E4.2 <- update(lmm.P1.E4.1,
                      formula = MeanAmp ~ Type * Category * Duration * ACC + 
                      (1 + Type + Category | SubjCode))

anova(lmm.P1.E4.2, lmm.P1.E4.1)
```


```{r}
lmm.P1.E4.3 <- update(lmm.P1.E4.2,
                      formula = MeanAmp ~ Type * Category * Duration * ACC + 
                      (1 + Type * Category | SubjCode))

anova(lmm.P1.E4.3, lmm.P1.E4.2)
```


```{r}
lmm.P1.E4.4 <- update(lmm.P1.E4.3,
                      formula = MeanAmp ~ Type * Category * Duration * ACC + 
                      (1 + Type * Category + Duration | SubjCode))
save(lmm.P1.E4.4, file = file.path(folder.nesi, "E204_P1_lmm_4.RData") )
anova(lmm.P1.E4.4, lmm.P1.E4.3)
```


```{r}
file.P1.E4.5 <- file.path(folder.nesi, "E204_P1_lmm_5.RData")

# fit the maximal model
if (!file.exists(file.P1.E4.5)) {
lmm.P1.E4.5 <- update(lmm.P1.E4.4,
                      formula = MeanAmp ~ Type * Category * Duration * ACC + 
                      (1 + Type * Category * Duration | SubjCode),
                      control = lmerControl(optCtrl = list(maxfun = 1e7)))
} else {
  load(file.P1.E4.5)
}
summary(lmm.P1.E4.5)
anova(lmm.P1.E4.5, lmm.P1.E4.4)
```


```{r}
file.P1.E4.6 <- file.path(folder.nesi, "E204_P1_lmm_6.RData")

# fit the maximal model
if (!file.exists(file.P1.E4.6)) {
lmm.P1.E4.6 <- update(lmm.P1.E4.5,
                      formula = MeanAmp ~ Type * Category * Duration * ACC + 
                      (1 + Type * Category * Duration + ACC | SubjCode),
                      control = lmerControl(optCtrl = list(maxfun = 1e7)))
} else {
  load(file.P1.E4.6)
}
anova(lmm.P1.E4.6, lmm.P1.E4.5)
```



#### The maximal model
```{r maximal lmm for E204 p1}
file.max.P1.E4 <- file.path(folder.nesi, "E204_P1_lmm_max.RData")

# fit the maximal model
if (!file.exists(file.max.P1.E4)) {
  lmm.max.P1.E4 <- lmer(MeanAmp ~ Type * Category * Duration * ACC + 
                          (1 + Type_D + Cate_D + Dura_D + Hemi_D + ACC_D + 
                             Type_Cate + Type_Dura + Cate_Dura + Type_Hemi + Cate_Hemi + Dura_Hemi + Type_ACC + Cate_ACC + Dura_ACC + Hemi_ACC +
                             Type_Cate_Dura + Type_Cate_Hemi + Type_Dura_Hemi + Cate_Dura_Hemi + Type_Cate_ACC + Type_Dura_ACC + Cate_Dura_ACC +
                             Type_Hemi_ACC + Cate_Hemi_ACC + Dura_Hemi_ACC +
                             Type_Cate_Dura_Hemi + Type_Cate_Dura_ACC + Type_Cate_Hemi_ACC + Type_Hemi_Dura_ACC + Hemi_Cate_Dura_ACC +
                             Type_Cate_Dura_Hemi_ACC| SubjCode),
                        data = df.erp.P1.E4,
                        REML = FALSE,
                        # verbose = TRUE,
                        control = lmerControl(optimizer = "bobyqa",  # nloptwrap Nelder_Mead
                                              optCtrl = list(maxfun = 1e7)))
} else {
  load(file.max.P1.E4)
}

print(summary(lmm.max.P1.E4), corr = FALSE)

```

#### The zero-correlation-parameter model
```{r zcp lmm for E204 p1}
# file.zcp.P1.E4 <- file.path(folder.nesi, "E204_P1_lmm_zcp.RData")
# 
# # fit the zcp model
# if (!file.exists(file.zcp.P1.E4)) {
#   lmm.zcp.P1.E4 <- update(lmm.max.P1.E4, 
#                           formula = MeanAmp ~ Type * Category * Duration * ACC + 
#                           (1 + Type_D + Cate_D + Dura_D + Hemi_D + ACC_D + 
#                              Type_Cate + Type_Dura + Cate_Dura + Type_Hemi + Cate_Hemi + Dura_Hemi + Type_ACC + Cate_ACC + Dura_ACC + Hemi_ACC +
#                              Type_Cate_Dura + Type_Cate_Hemi + Type_Dura_Hemi + Cate_Dura_Hemi + Type_Cate_ACC + Type_Dura_ACC + Cate_Dura_ACC +
#                              Type_Hemi_ACC + Cate_Hemi_ACC + Dura_Hemi_ACC +
#                              Type_Cate_Dura_Hemi + Type_Cate_Dura_ACC + Type_Cate_Hemi_ACC + Type_Hemi_Dura_ACC + Hemi_Cate_Dura_ACC +
#                              Type_Cate_Dura_Hemi_ACC || SubjCode),
#                           verbose = FALSE)
# } else {
#   load(file.zcp.P1.E4)
# }
# 
# print(summary(lmm.zcp.P1.E4), corr = FALSE)

```

```{r compare max and zcp E204 P1}
anova(lmm.max.P1.E4, lmm.zcp.P1.E4)
```

#### The reduced model
```{r PCA analysis for zcp lmm for E204 P1}

# file.zcp.P1.E4.step <- file.path(folder.nesi, "E204_P1_lmm_zcp_step.RData")
# 
# # fit the zcp model
# if (!file.exists(file.zcp.P1.E4)) {
#   lmm.zcp.P1.E4.step <- step(lmm.zcp.P1.E4, reduce.fixed = FALSE)
# } else {
#   load(file.zcp.P1.E4.step)
# }
# 
# summary(lmm.zcp.P1.E4.step)
```


```{r get model for P1 E4}
lmm.rdc.P1.E4 <- get_model(lmm.zcp.P1.E4.step)

print(summary(lmm.rdc.P1.E4), corr = FALSE)
```


#### Extended model
```{r }


```
At least three random factors should be removed. (Cate_D, Type_Cate, Cate_Dura)


```{r }
# lmm.etd.P1.E4 <- update(lmm.max.P1.E4,
#                         formula = MeanAmp ~ Type * Category * Duration + 
#                           (1 + Type_D + Dura_D + Type_Dura + Type_Cate_Dura | SubjCode),
#                         verbose = FALSE)
# 
# summary(lmm.etd.P1.E4)
```



### Ampitudes of the N170
```{r erp df for the N170}
# only keep the data for amplitudes of the N170 (all erps)
df.erp.N170.E4 <- {
  df.erp.E4 %>% 
    filter(Component == "N170", ACC != "all") %>% 
    substr_Event() 
}

if (saveCSV) {
  output.erp.N170.E4 <- file.path(folder.nesi, "E204_erp_N170.RData")
  save(df.erp.N170.E4, file = output.erp.N170.E4)
}

# the structure of this dataset
head(df.erp.N170.E4, 10)
```

#### The maximal model
```{r maximal lmm for E204 N170}
file.max.N170.E4 <- file.path(folder.nesi, "E204_N170_lmm_max.RData")

# fit the maximal model
if (!file.exists(file.max.N170.E4)) {
  lmm.max.N170.E4 <- lmer(MeanAmp ~ Type * Category * Duration * ACC + 
                          (1 + Type_D + Cate_D + Dura_D + Hemi_D + ACC_D + 
                             Type_Cate + Type_Dura + Cate_Dura + Type_Hemi + Cate_Hemi + Dura_Hemi + Type_ACC + Cate_ACC + Dura_ACC + Hemi_ACC +
                             Type_Cate_Dura + Type_Cate_Hemi + Type_Dura_Hemi + Cate_Dura_Hemi + Type_Cate_ACC + Type_Dura_ACC + Cate_Dura_ACC +
                             Type_Hemi_ACC + Cate_Hemi_ACC + Dura_Hemi_ACC +
                             Type_Cate_Dura_Hemi + Type_Cate_Dura_ACC + Type_Cate_Hemi_ACC + Type_Hemi_Dura_ACC + Hemi_Cate_Dura_ACC +
                             Type_Cate_Dura_Hemi_ACC | SubjCode),
                        data = df.erp.N170.E4,
                        REML = FALSE,
                        # verbose = TRUE,
                        control = lmerControl(optimizer = "bobyqa",  # nloptwrap Nelder_Mead
                                              optCtrl = list(maxfun = 1e7)))
} else {
  load(file.max.N170.E4)
}

print(summary(lmm.max.N170.E4), corr = FALSE)

```

#### The zero-correlation-parameter model
```{r zcp lmm for E204 N170}

# file.zcp.N170.E4 <- file.path(folder.nesi, "E204_N170_lmm_zcp.RData")
# 
# # fit the zcp model
# if (!file.exists(file.zcp.N170.E4)) {
#   lmm.zcp.N170.E4 <- update(lmm.max.N170.E4, 
#                             formula = MeanAmp ~ Type * Category * Duration * ACC + 
#                           (1 + Type_D + Cate_D + Dura_D + Hemi_D + ACC_D + 
#                              Type_Cate + Type_Dura + Cate_Dura + Type_Hemi + Cate_Hemi + Dura_Hemi + Type_ACC + Cate_ACC + Dura_ACC + Hemi_ACC +
#                              Type_Cate_Dura + Type_Cate_Hemi + Type_Dura_Hemi + Cate_Dura_Hemi + Type_Cate_ACC + Type_Dura_ACC + Cate_Dura_ACC +
#                              Type_Hemi_ACC + Cate_Hemi_ACC + Dura_Hemi_ACC +
#                              Type_Cate_Dura_Hemi + Type_Cate_Dura_ACC + Type_Cate_Hemi_ACC + Type_Hemi_Dura_ACC + Hemi_Cate_Dura_ACC +
#                              Type_Cate_Dura_Hemi_ACC || SubjCode),
#                             verbose = FALSE)
# } else {
#   load(file.zcp.N170.E4)
# }
# 
# 
# print(summary(lmm.zcp.N170.E4), corr = FALSE)

```

```{r compare max and zcp E204 N170}
anova(lmm.max.N170.E4, lmm.zcp.N170.E4)
```

#### The reduced model
```{r PCA analysis for zcp lmm for E204 N170}

# file.zcp.N170.E4.step <- file.path(folder.nesi, "E204_N170_lmm_zcp_step.RData")
# 
# # fit the zcp model
# if (!file.exists(file.zcp.N170.E4)) {
#   lmm.zcp.N170.E4.step <- step(lmm.zcp.N170.E4, reduce.fixed = FALSE)
# } else {
#   load(file.zcp.N170.E4.step)
# }
# 
# lmm.zcp.N170.E4.step
```


```{r get model for N170 E4}
lmm.rdc.N170.E4 <- get_model(lmm.zcp.N170.E4.step)

print(summary(lmm.rdc.N170.E4), corr = FALSE)
```


#### Extended model
```{r PCA analysis for zcp lmm for E204 N170}


```

#205
## Behavior results
```{r load behavioral results of 205}
raw.beha.E5 <- {
  "12.csv" %>% 
    file.path(folder.data, .) %>% 
    read_csv() 
}

```

```{r select certain rows and columns from the data, and calculate the Z value for reaction times (205)}
clean.beha.E5 <- {
  raw.beha.E5 %>%
    filter(`Procedure[Trial]` == "expProc") %>% # remove rows for practice and countdown
    select(ExperimentName, SubjCode = Subject, Block, Trial, Age, Sex, Handedness, Type = `stimCategory[Trial]`, Category = `FH[Trial]`, Duration = `stimDuration[Trial]`, ACC = `Resp.ACC[Trial]`, Resp.RT = `Resp.RT[Trial]`, Resp = `Resp.RESP[Trial]`, Stimuli = `stimName[Trial]`) %>% # select and rename the columns (remove unuseful columns)
    mutate(
      SubjCode = factor(paste("P", SubjCode, sep = "")), # save SubjCode as factor
      Type = substr(Type, 1, 1),
      Type = recode_factor(Type, "N" = "normal", "S" = "scrambled"), # rename the levels of Type
      Category = recode(Category, "hosue" = "house", "house" = "house", "face" = "face", .default = "face"), # rename "hosue" as "house"
      Duration = factor(Duration) # save Duration as factor
      ) %>% 
    group_by(SubjCode, Type, Category, Duration, ACC) %>% # divide the data based on these conditions
    mutate(
      RT = Resp.RT + as.numeric(levels(Duration))[Duration],
      RT.Z = scale(RT), # calculate the Z value for reaction times within each group
      RT.Within3Z = ifelse(RT.Z <= 3 & RT.Z >= -3, 1, ifelse(RT.Z < -3 | RT.Z > 3, 0, NaN)) # if the Z values are between -3 and 3, will be marked as 1
      ) %>% 
    ungroup() # ungroup the data 
     
}

# behavior.tidyup
head(clean.beha.E5, 10)

```

```{r check the number of trials for 205}
# check the number of trials for 205
sum.count.E5 <- {
  clean.beha.E5 %>% 
    group_by(SubjCode, Type, Category, Duration) %>% 
    summarize(Count_sum = n())
}

valid.count.E5 <- {
  clean.beha.E5 %>% 
    filter(RT > RT.min) %>% 
    group_by(SubjCode, Type, Category, Duration) %>% 
    summarize(Count = n()) %>% 
    right_join(sum.count.E5, by = c("SubjCode", "Type", "Category", "Duration")) %>% 
    mutate(ratio = Count / Count_sum) %>% 
    filter(ratio > ratio.count) %$%
    SubjCode %>% 
    unique()
}


# remove the participants
clean.beha.E5 %<>% filter(SubjCode %in% valid.count.E5)

```


```{r remove participants whose performance in Key 1 and Key 5 conditions were lower than 90%}
# remove participants whose performance in Key 1 and Key 5 conditions were lower than 90%

valid.17 <- {
  clean.beha.E5 %>%
    filter(Type == "normal" & Duration == 17 & Resp == 1) %>% 
    group_by(SubjCode, Type, Duration, Resp) %>% 
    summarize(Accuracy = mean(ACC), Count = n()) %>% 
    filter(Accuracy > 0.9) %$%
    SubjCode
}

clean.beha.E5 %<>% filter(SubjCode %in% valid.17)

```

### Participant demographic information
```{r participant information}
subj.info.E5 <- {
  clean.beha.E5 %>% 
    select(SubjCode, Age, Sex, Handedness) %>% # select the columns of participant inforamtion
    distinct() # only keep the unique rows
}

subj.info.E5
```


# Questions and predictions {#prediction}

** Predictions **

* P1. The N170 amplitudes of normal faces are larger than those of houses.
* P2. The N170 amplitudes of the right hemisphere are larger than those of the left hemisphere. 
* P3. The differences of the N170 amplitudes between left and right hemisphere for faces are larger than those for houses.
* P4. There should be no differences among the conditions for scrambled stimuli.


# Appendix

## Model selection procedure

For the analysis for every dependent variable, an optimal model was obtained by the following general steps:
  
1. **Maximum Model**: a maximum model with all fixed and random factors was built. 
2. **ZCP Model**: a zero-correlated-parameter (zcp) model based on the maximum model is built. The only difference between zcp and maximum models is that the correlations between random effects are forced to be zero in the zcp model.
3. **Reduced Model**: the random effects which are not supported by the data will be removed from the zcp model. The function `step(fit, fixed.reduce = FALSE)` from `library(lmerTest)` is used for this step. The algorithm could be found [here](#stepfun).
4. **Extended Model**: extending the reduced model with correlation parameters between the remaining random effects.
5. **Pruning Model**: pruning low correlation parameters. (Usually I don't do this.)
  
More details about this whole process chould be found here: Bates, D., Kliegl, R., Vasishth, S., & Baayen, H. (2015). Parsimonious Mixed Models. Retrieved from http://arxiv.org/abs/1506.04967.

## Algorithm of function `step` {#stepfun}
The outline of the algorithm of `step(fit, reduce.fixed = false)` function is: 

>  Simplification of the random effects structure:
>
>   1. Let M be the linear mixed effects model specified by a user. 
>
>   2. If there are random effects in M then go to 3, otherwise stop. 
>
>   3. For each random effect ri in M do: 
>       (a) Create a reduced model Mi by eliminating ri from M. 
>
>       (b) Calculate pi, the p value from the likelihood ratio test of comparing M to Mi. 
>
>       (c) Save pi and Mi.
>
>   4. Find pmax; the maximum of all pi and let Mmax denote the corresponding model. 
>
>   5. Set M to Mmax. If pmax is higher than Î± level then go back to 3, otherwise stop.

More deatils could be found: Kuznetsova, A., Brockhoff, P. B., & Christensen, R. H. B. (2017). lmerTest Package: Tests in Linear Mixed Effects Models. Journal of Statistical Software, 82(13), 1-26. http://doi.org/10.18637/jss.v082.i13 **Page 8**



# Versions of packages used
```{r versions}
rstudioapi::versionInfo()
sessionInfo()
```
